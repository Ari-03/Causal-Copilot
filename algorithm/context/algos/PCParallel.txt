
────────────────────────────────────────────────────────────────────────
1. HYPER-PARAMETERS SENSITIVITY
────────────────────────────────────────────────────────────────────────
• Number of Key Hyperparameters  
  - The PC algorithm has three main hyperparameters:  
    1. α (alpha): Significance level for conditional independence tests.  
    2. indep_test: Choice of conditional independence test (e.g., Fisher’s Z, chi-square, G-square).  
    3. cores: Number of CPU cores to be utilized for parallel computation. 
    4. memory_efficient: Boolean variable to use a more memory efficient computation algorithm, tradeoff with running time. 

• Tuning Difficulty  
  - α is typically the most critical parameter, with recommended defaults in the provided file (e.g., 0.05 for moderate sample sizes, 0.1 for smaller samples, 0.01 for very large samples). These guidelines simplify tuning for different dataset sizes.  
  - The choice of indep_test can be straightforward (Fisher’s Z for continuous data, chi-square for discrete data, etc.). Current implementation uses only the mentioned independence tests.
  - cores should ideally be set based on hardware availability (e.g., setting it too high may cause thread contention).
  - memory_efficient mode should only be used for very large datasets with large number of nodes (e.g. number of variables > 2000 and the number of available samples are > 500)

• Sensitivity  
  - Small changes in α can produce noticeable effects on edge detection: lower α yields more conservative edge selection (fewer false positives) but may lead to more false negatives.  
  - Using too few threads results in minimal speedup, while excessive parallelization may cause memory bottlenecks.
  - For very large datasets, where memory usage could be an issue, a tradeoff can be made on runtime to utilize the memory more efficiently.

• Critique/Extension  
  - Parallelization improves runtime but does not mitigate statistical limitations (e.g., incorrect edge detection due to faithfulness violations).
  - Performance benefits are hardware-dependent—some systems experience diminishing returns beyond a certain number of threads.

────────────────────────────────────────────────────────────────────────
2. ROBUSTNESS & SCALABILITY
────────────────────────────────────────────────────────────────────────
• Tolerance to Bad Data Quality  
  - Parallel-PC inherits PC’s lack of built-in missing data handling.
  - Errors in CI tests are not corrected by parallelization, meaning missing or noisy data still degrade performance. 

• Tolerance to Sparse vs. Dense Connected Systems  
  - Parallelization helps in denser graphs, where the number of CI tests grows combinatorially.
  - Benchmarks suggest speed gains of up to 10× on dense networks, but correctness depends on CI test quality.

• Scalability  
  - Can handle higher-dimensional problems than standard PC.
  - Linear scalability up to a certain number of cores (12-14), after which performance gains diminish due to memory overhead.

• Critique/Extension  
  - Unlike the standard PC, Parallel-PC is explicitly designed for large graphs.
  - Despite parallelism, datasets with thousands of variables may still require heuristic modifications like early stopping or approximate skeleton pruning.
  
────────────────────────────────────────────────────────────────────────
3. MIXED DATA & COMPLEX FUNCTIONS
────────────────────────────────────────────────────────────────────────
• Noise Type  
  - The PC algorithm’s traditional setup often presupposes approximately Gaussian errors for continuous data, but other independence tests allow for broader noise distributions.  
  - Benchmarks (File #2) suggested that PC performed somewhat lower when confronted with more diverse or non-Gaussian noise, although advanced tests (KCI, RCIT) can address non-linearity.  

• Mixed Data (Continuous & Discrete)  
  - PC can integrate different conditional independence tests: Fisher’s Z or partial correlation for continuous variables; chi-square or G-square for discrete; or a mix for hybrid data.  
  - Some PC implementations provide direct support for mixed data via specialized tests like G-square, allowing simultaneous handling of continuous and categorical variables.  

• Heterogeneous Data  
  - Benchmark results (File #2) show it scored in the middle ranges under “Heterogeneity.” It can handle moderate changes in distribution across variables, provided a suitable test is chosen.  
  - Marked shifts across subsets of data may require domain-driven adjustments or advanced versions (like FCI for partial latent confounding).  

• Complex Functional Forms  
  - The standard PC algorithm is tailored for linear (or near-linear) dependencies, but employing a kernel-based test (e.g., KCI) can detect non-linear relationships.  
  - Non-linear extensions usually come with higher computational burdens, which can be partially offset by “fastKCI” or “RCIT.”  

• Critique/Extension  
  - Users often underestimate how crucial the independence test choice is when dealing with non-linear patterns. A robust non-parametric test can significantly improve detection of non-linear causal links.  
  - Overfitting concerns can arise in small samples if a highly flexible test is used without proper regularization or multiple testing corrections.  

────────────────────────────────────────────────────────────────────────
4. COMPUTATIONAL COMPLEXITY
────────────────────────────────────────────────────────────────────────
• Theoretical Time Complexity  
  - The worst-case complexity of PC is <temp>[O(n^(k+2))]</temp>, parallelization does not change this, but distributes the workload to achieve better practical runtime.

• Variability in Practical Usage  
  - On modern multi-core CPUs, speed improvements scale well up to 16–32 cores, after which performance gains saturate.
  - memory_efficient mode can be deployed for very large datasets, trading off some runtime over memory consumption. 

• Critique/Extension  
  - Parallelization does not reduce worst-case complexity, but makes real-world execution feasible for larger graphs.
  - Works best when CPU utilization is optimized—thread scheduling inefficiencies can arise with suboptimal configurations.

────────────────────────────────────────────────────────────────────────
5. INTERPRETABILITY
────────────────────────────────────────────────────────────────────────
• Output Format  
  - Parallel-PC outputs a CPDAG (Completed Partially Directed Acyclic Graph). This graph shows which edges are definitely directed, as well as edges uncertain about orientation.  

• Strength of the Output Format  
  - CPDAGs are considered quite interpretable: they clarify which directions are inferred and which remain undetermined.  
  - Many software packages (e.g., Tetrad, pcalg) provide visualization tools to help interpret a CPDAG.  

• Limitations of the Output Format  
  - Edges in the “partially” directed region can generate ambiguity, as the algorithm leaves some edges unoriented if the data are insufficiently informative.  
  - In high-dimensional or complex domains, the resulting graph might still be large and cumbersome to interpret without additional domain knowledge.  

• Critique/Extension  
  - Domain experts frequently refine or manually orient ambiguous edges, especially in specialized fields such as genomics or neuroscience.  
  - Some suggest performing a stability or bootstrap analysis to highlight edges that are consistently inferred across subsamples.  

────────────────────────────────────────────────────────────────────────
6. ASSUMPTIONS
────────────────────────────────────────────────────────────────────────
• Critical Assumptions  
  - Causal Markov condition: Each variable is conditionally independent of its non-descendants, given its parents.  
  - Faithfulness: All and only the conditional independencies in the true structure are reflected in the data.  
  - Causal sufficiency: No unobserved confounders of the measured variables.  
  - Acyclicity: The underlying causal structure forms a Directed Acyclic Graph (DAG).  

• Violation Impact  
  - Missing confounders can lead to spurious or missing edges.  
  - Unfaithful data-generating mechanisms can hide true edges or create extra independencies.  
  - Feedback loops or cyclic structures break the acyclicity assumption, causing the algorithm to infer incomplete or incorrect edges.  

• Critique/Extension  
  - When hidden variable confounding is suspected, variants like FCI (Fast Causal Inference) or RFCI are recommended instead of basic PC.  
  - Many real-world systems include some feedback or non-stationary behavior, so partial violations are not uncommon. Users often need to test sensitivity or adopt extended algorithms.  

────────────────────────────────────────────────────────────────────────
7. (OPTIONAL) REAL-WORLD BENCHMARKS
────────────────────────────────────────────────────────────────────────
• Performance on Real Datasets  
  - Parallel-PC demonstrates significantly better runtime than standard PC, with similar accuracy. Gains are most prominent in high-dimensional datasets, where speedup factors of 5× to 10× are common.
  - Since Parallel-PC implements the PC-stable version of the algorithm, the performance will be similar if not better than traditional PC. 

• Practical Tips  
  - Combining PC with domain knowledge (e.g., known biological pathways) often yields improved orientation of edges.  
  - Ensure correct number of cores—too many parallel cores may cause contention.
  - Discussion forums and user groups consistently recommend verifying the sample size is sufficient for the chosen α level, or considering small-sample corrections if data are limited.  

────────────────────────────────────────────────────────────────────────
CONCLUSION
────────────────────────────────────────────────────────────────────────
Parallel-PC retains the interpretability and theoretical foundations of the standard PC algorithm while significantly improving scalability and runtime through parallelization of CI tests. The key benefits include:
 • Faster execution on multi-core CPUs.
 • Better handling of large graphs, particularly dense structures.
 • Maintains correctness guarantees from PC, but does not solve its statistical limitations.
However, Parallel-PC still faces similar challenges to standard PC, such as:
 • High sensitivity to significance thresholds (α).
 • Limited robustness to missing data and confounding.
 • No explicit handling of cyclic or feedback systems.
────────────────────────────────────────────────────────