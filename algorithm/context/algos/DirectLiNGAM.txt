
# InterIAMB Algorithm Profile

## Executive Summary
- **Optimal use scenarios**: Moderate-sized datasets (≤30 variables, 500-10,000 samples) with primarily linear relationships and minimal missing data; feature selection tasks where identifying minimal predictive variable sets is the goal.
- **Alternative recommendations**: For non-linear data, prefer CDNOD with RCIT; for high-dimensional data, consider GRaSP or PC with Fisher Z; for data with latent confounders, use FCI variants.

## 1. Real-World Applications
- **Best Use Cases**: 
  - Feature selection for classification tasks in biomedical data (Tsamardinos et al., 2003)
  - Variable selection in spectroscopic analysis (Cui et al., 2017)
  - Structural learning of moderate-sized Bayesian networks (Aliferis et al., 2010)
- **Limitations**: 
  - Poor performance with very high-dimensional data (>50 variables)
  - Ineffective for data with significant hidden confounders
  - Not suitable for time-critical applications with large datasets
  - Struggles with complex non-linear systems without specialized tests

## 2. Assumptions
- **Core theoretical assumptions**: Causal sufficiency (no significant unobserved confounders); Markov condition (variable independent of non-descendants given parents); faithfulness (observed independencies reflect true causal structure).
- **Effects of assumption violations**: Performance deteriorates significantly when causal sufficiency is violated; sensitive to faithfulness violations resulting in spurious or missing edges.

## 3. Data Handling Capabilities
- **Performance across data types**: Good with linear data using "fisherz" test; handles pure discrete data with "chisq" test; struggles with mixed data types without specialized tests.
- **Handling of relationship complexity**: Poor performance with non-linear relationships using default settings; can handle non-linearity when using appropriate tests (kci/fastkci/rcit) but with significant runtime cost.
- **Noise tolerance**: Moderate tolerance to Gaussian noise; less robust to non-Gaussian noise distributions.

## 4. Robustness & Scalability
- **Missing data tolerance**: Poor with missing rates >10%; performance degrades rapidly as missing data increases.
- **Measurement error resilience**: Moderate for low error levels (<0.3); significantly degrades with higher rates.
- **Network density performance**: Better with moderately dense networks (edge probability ~0.2); less effective for very sparse networks.
- **Variable and sample scaling**: Handles up to 30 variables efficiently; requires 500-10,000 samples for reliable results; performance suffers outside these ranges.
- **Multi-domain data handling**: Limited capability for multi-domain data without modifications.

## 5. Computational Complexity
- **Theoretical time complexity**: O(n²) in best case, O(n² × m × f(d)) in practice, where n=variables, m=samples, d=graph degree.
- **Practical runtime characteristics**: Fast for small datasets (<15 variables, <1000 samples); significant slowdown with non-linear independence tests.
- **Hardware requirements**: Standard CPU sufficient; no specialized hardware needed unless using advanced tests.

## 6. Hyperparameters
- **Key hyperparameters**: alpha (significance level); indep_test (independence test method)
- **Default performance**: Adequate in standard scenarios with default settings (α=0.05, indep_test="fisherz")
- **Tuning difficulty**: Moderate; clear heuristics exist for parameter selection based on sample size and data type
- **Impact of incorrect settings**: Inappropriate independence test can miss critical relationships; overly strict/lenient alpha values lead to missing edges or false positives

## 7. Interpretability
- **Output format**: Returns Markov blanket (parents, children, and spouses) for each target variable
- **Confidence measures**: No native confidence metrics for discovered relationships
- **Ambiguity handling**: Does not inherently resolve causal direction ambiguities
