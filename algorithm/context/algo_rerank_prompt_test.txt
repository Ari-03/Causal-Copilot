You will conduct causal discovery on the Tabular Dataset ./demo_data/20250322_134744/Abalone/Abalone.csv containing the following Columns:

Height	Age	Whole_weight	Diameter	Length	Shell_weight

The Detailed Background Information is listed below:

This is fake domain knowledge for debugging purposes.

The Statistics Information about the dataset is:

The dataset has the following characteristics:

Data Type: The overall data type is Continuous.

The sample size is 4177 with 6 features. This dataset is not time-series data. Data Quality: There are no missing values in the dataset.

Statistical Properties:
- Linearity: The relationships between variables are not predominantly linear.
- Gaussian Errors: The errors in the data do not follow a Gaussian distribution.
- Heterogeneity: The dataset is not heterogeneous. 





Based on the above information, please select the best-suited algorithm from the following candidate (the order of the algorithm candidates is not important):

dict_keys(['PC', 'FCI', 'GES'])

===============================================
Note that the user can only wait for 1440.0 minutes for the algorithm execution, please ensure the time cost of the selected algorithm would not exceed it!
The estimated time costs of the following algorithms are below. Consider the time cost wisely when selecting the algorithm, it is critical but less important than the performance when the time cost does not exceed the waiting time, the importance ratio to the performance is 1:2:

PC: 0.2 minutes
FCI: 0.2 minutes
GES: 0.2 minutes

===============================================
Detailed Profiles of the algorithm candidates are shown here. You MUST actively combine and reason with them:

# PC

Below is a seven-dimension profile of the PC (Peter–Clark) algorithm, integrating (1) the hyperparameter file, (2) the benchmarking results, and (3) external knowledge about the algorithm and its typical use cases. References or paraphrased remarks from the external sources are indicated where relevant.

────────────────────────────────────────────────────────────────────────
1. HYPER-PARAMETERS SENSITIVITY
────────────────────────────────────────────────────────────────────────
• Number of Key Hyperparameters  
  - The PC algorithm has three main hyperparameters:  
    1. α (alpha): Significance level for conditional independence tests.  
    2. indep_test: Choice of conditional independence test (e.g., Fisher’s Z, chi-square, G-square, KCI variants).  
    3. depth: Maximum depth for the skeleton search phase.  

• Tuning Difficulty  
  - α is typically the most critical parameter, with recommended defaults in the provided file (e.g., 0.05 for moderate sample sizes, 0.1 for smaller samples, 0.01 for very large samples). These guidelines simplify tuning for different dataset sizes.  
  - The choice of indep_test can be straightforward (Fisher’s Z for continuous data, chi-square for discrete data, etc.), though advanced options (like KCI) may require more specialized knowledge.  
  - depth has a default of -1 (unlimited), but it can be restricted to reduce runtime for large graphs. The suggested rule of thumb scales with the number of nodes, setting smaller depths for larger graphs.  

• Sensitivity  
  - Small changes in α can produce noticeable effects on edge detection: lower α yields more conservative edge selection (fewer false positives) but may lead to more false negatives.  
  - Shifting from an unlimited depth (depth = -1) to a restricted depth (like 1 to 3) can significantly speed up the search on large graphs but may miss subtler causal relationships.  
  - Changing the independence test (e.g., from a linear Fisher test to a non-parametric test like KCI) can likewise alter both runtime and the ability to capture non-linear causal links.  

• Critique/Extension  
  - Parameters that control graph-search complexity (depth) can drastically reduce runtime but may compromise completeness in highly connected graphs.  
  - The α threshold has more direct influence on statistical testing; even small shifts in α can change the number of edges found. Hence, domain knowledge is often helpful to choose a good α.  

────────────────────────────────────────────────────────────────────────
2. ROBUSTNESS & SCALABILITY
────────────────────────────────────────────────────────────────────────
• Tolerance to Bad Data Quality  
  - Benchmarks (File #2) indicate that PC’s performance tends to degrade when data quality drops (e.g., missing data or measurement error). In fact, PC ranked relatively low in measurement error and missing data tolerance, suggesting it does not handle either type of bad data as robustly as some other methods.  
  - It lacks built-in methods for imputation; in practice, users might preprocess missing values or use more advanced PC variants (e.g., PC-missing).  

• Tolerance to Sparse vs. Dense Connected Systems  
  - PC often performs better on sparse systems, since the number of conditional independence tests remains more manageable. For dense graphs, the algorithm may require many tests, increasing the risk of both false positives and elevated runtime.  
  - From a benchmarking standpoint, when confronted with moderately dense networks, performance declines more in runtime than in accuracy, highlighting the combinatorial explosion of tests in denser graphs.  

• Scalability  
  - According to File #2, the algorithm scored moderately on scalability, reflecting that it can handle dozens to a few hundred variables well, but may slow down noticeably beyond that range.  
  - Parallelization or restricting depth can mitigate the combinatorial explosion in larger problems but may reduce the completeness of causal edges.  

• Critique/Extension  
  - Parallel-PC implementations exist and can be employed on multi-core hardware to improve speed.  
  - Using approximation strategies (like a lower maximum depth or partitioning variables) can be beneficial when the dataset contains hundreds or thousands of variables, though these techniques might weaken correctness guarantees.  

────────────────────────────────────────────────────────────────────────
3. MIXED DATA & COMPLEX FUNCTIONS
────────────────────────────────────────────────────────────────────────
• Noise Type  
  - The PC algorithm’s traditional setup often presupposes approximately Gaussian errors for continuous data, but other independence tests allow for broader noise distributions.  
  - Benchmarks (File #2) suggested that PC performed somewhat lower when confronted with more diverse or non-Gaussian noise, although advanced tests (KCI, RCIT) can address non-linearity.  

• Mixed Data (Continuous & Discrete)  
  - PC can integrate different conditional independence tests: Fisher’s Z or partial correlation for continuous variables; chi-square or G-square for discrete; or a mix for hybrid data.  
  - Some PC implementations provide direct support for mixed data via specialized tests like G-square, allowing simultaneous handling of continuous and categorical variables.  

• Heterogeneous Data  
  - Benchmark results (File #2) show it scored in the middle ranges under “Heterogeneity.” It can handle moderate changes in distribution across variables, provided a suitable test is chosen.  
  - Marked shifts across subsets of data may require domain-driven adjustments or advanced versions (like FCI for partial latent confounding).  

• Complex Functional Forms  
  - The standard PC algorithm is tailored for linear (or near-linear) dependencies, but employing a kernel-based test (e.g., KCI) can detect non-linear relationships.  
  - Non-linear extensions usually come with higher computational burdens, which can be partially offset by “fastKCI” or “RCIT.”  

• Critique/Extension  
  - Users often underestimate how crucial the independence test choice is when dealing with non-linear patterns. A robust non-parametric test can significantly improve detection of non-linear causal links.  
  - Overfitting concerns can arise in small samples if a highly flexible test is used without proper regularization or multiple testing corrections.  

────────────────────────────────────────────────────────────────────────
4. COMPUTATIONAL COMPLEXITY
────────────────────────────────────────────────────────────────────────
• Theoretical Time Complexity  
  - The PC algorithm can have a worst-case time complexity of <temp>[O(n^(k+2))]</temp>, where n is the number of variables and k is the maximum degree of a node in the true graph.  

• Variability in Practical Usage  
  - Despite the polynomial (sometimes high-order) worst-case bound, many real-world graphs are sparse, so runtime is often much lower.  
  - Increasing depth or lowering α can drive up the number of tests, and thus runtime can spike. Conversely, restricting depth dampens the combinatorial explosion but risks missing some edges.  

• Critique/Extension  
  - The worst-case scenario might be rarely encountered in practical sparse settings. However, in denser structures or with large depth parameters, runtime can indeed grow significantly.  
  - Parallelization can help distribute the skeleton-discovery phase across multiple CPUs, improving the typical runtime on large datasets.  

────────────────────────────────────────────────────────────────────────
5. INTERPRETABILITY
────────────────────────────────────────────────────────────────────────
• Output Format  
  - PC outputs a CPDAG (Completed Partially Directed Acyclic Graph). This graph shows which edges are definitely directed, as well as edges uncertain about orientation.  
  - Some implementations also provide adjacency matrices with confidence measures (e.g., p-values).  

• Strength of the Output Format  
  - CPDAGs are considered quite interpretable: they clarify which directions are inferred and which remain undetermined.  
  - Many software packages (e.g., Tetrad, pcalg) provide visualization tools to help interpret a CPDAG.  

• Limitations of the Output Format  
  - Edges in the “partially” directed region can generate ambiguity, as the algorithm leaves some edges unoriented if the data are insufficiently informative.  
  - In high-dimensional or complex domains, the resulting graph might still be large and cumbersome to interpret without additional domain knowledge.  

• Critique/Extension  
  - Domain experts frequently refine or manually orient ambiguous edges, especially in specialized fields such as genomics or neuroscience.  
  - Some suggest performing a stability or bootstrap analysis to highlight edges that are consistently inferred across subsamples.  

────────────────────────────────────────────────────────────────────────
6. ASSUMPTIONS
────────────────────────────────────────────────────────────────────────
• Critical Assumptions  
  - Causal Markov condition: Each variable is conditionally independent of its non-descendants, given its parents.  
  - Faithfulness: All and only the conditional independencies in the true structure are reflected in the data.  
  - Causal sufficiency: No unobserved confounders of the measured variables.  
  - Acyclicity: The underlying causal structure forms a Directed Acyclic Graph (DAG).  

• Violation Impact  
  - Missing confounders can lead to spurious or missing edges.  
  - Unfaithful data-generating mechanisms can hide true edges or create extra independencies.  
  - Feedback loops or cyclic structures break the acyclicity assumption, causing the algorithm to infer incomplete or incorrect edges.  

• Critique/Extension  
  - When hidden variable confounding is suspected, variants like FCI (Fast Causal Inference) or RFCI are recommended instead of basic PC.  
  - Many real-world systems include some feedback or non-stationary behavior, so partial violations are not uncommon. Users often need to test sensitivity or adopt extended algorithms.  

────────────────────────────────────────────────────────────────────────
7. (OPTIONAL) REAL-WORLD BENCHMARKS
────────────────────────────────────────────────────────────────────────
• Performance on Real Datasets  
  - Across a variety of real datasets (e.g., gene-expression data, brain connectivity data), PC tends to perform competitively for structure learning, provided the data are not extremely noisy or riddled with missing values.  
  - In some evaluations, PC was among the more accurate approaches for sparse graphs but was sometimes outperformed by score-based or hybrid methods on very noisy or dense data.  

• Practical Tips  
  - Combining PC with domain knowledge (e.g., known biological pathways) often yields improved orientation of edges.  
  - Limiting depth can drastically reduce computation time for high-dimensional data, but practitioners must confirm that the trade-off in missed edges is acceptable.  
  - Discussion forums and user groups consistently recommend verifying the sample size is sufficient for the chosen α level, or considering small-sample corrections if data are limited.  

────────────────────────────────────────────────────────────────────────
CONCLUSION
────────────────────────────────────────────────────────────────────────
The PC algorithm remains a flagship approach in constraint-based causal discovery, prized for its interpretability (CPDAG output) and relatively direct hyperparameter tuning guidelines (notably α and depth). It is best applied in scenarios where:  
• The graph is not excessively dense.  
• The data generally meet Markov and faithfulness assumptions.  
• There are no severe missing data or measurement error problems, or these issues are pre-processed/handled externally.  
Recent developments in parallelization, alternative independence tests, and specialized PC variants help address non-linearities, mixed data, and large-scale settings. Nonetheless, potential users must keep in mind the algorithm’s sensitivity to significance thresholds, data assumptions, and the combinatorial explosion of conditional tests in bigger or denser networks.────────────────────────────────────────────────────────
Benchmarking Results
────────────────────────────────────────────────────────

• Comparative Performance
  – The benchmarking compared 19 different causal discovery algorithms across multiple scenarios.
  – Each algorithm was evaluated on performance (accuracy), efficiency (runtime), and composite metrics.
  – Composite scores combine performance and efficiency (weights: 0.8 and 0.2 respectively).
  – Metrics are scored from 1-5, with 5 being the best.

• Algorithm Rankings

| Scenario | Mean Rank | Std Dev | Performance | Efficiency | Composite |
|----------|-----------|---------|-------------|------------|------------|
| Scalability | 7.4 | 3.00 | 2.0 | 3.0 | 2.0 |
| Heterogeneity | 5.5 | 4.33 | 2.0 | 2.0 | 2.0 |
| Measurement Error | 13.0 | 0.00 | 2.0 | 3.0 | 2.0 |
| Noise Type | 11.5 | 1.50 | 2.0 | 4.0 | 2.0 |
| Missing Data | 13.0 | 0.00 | 2.0 | 3.0 | 2.0 |

• Analysis

  – Overall mean ranking across 5 scenarios: 10.07
  – Average standard deviation: 1.77

• Key Observations
  – Moderately stable performance across scenarios
  – Best performance in Heterogeneity scenario (rank 5.5)

*Note: Rankings are relative positions among all tested algorithms, while level scores are absolute quantitized measures of capability.*


────────────────────────────────────────────────────────
Benchmarking Results
────────────────────────────────────────────────────────

• Comparative Performance
  – The benchmarking compared 19 different causal discovery algorithms across multiple scenarios.
  – Each algorithm was evaluated on performance (accuracy), efficiency (runtime), and composite metrics.
  – Composite scores combine performance and efficiency (weights: 0.8 and 0.2 respectively).
  – Metrics are scored from 1-5, with 5 being the best.

• Algorithm Rankings

| Scenario | Mean Rank | Std Dev | Performance | Efficiency | Composite |
|----------|-----------|---------|-------------|------------|------------|
| Scalability | 7.4 | 3.00 | 2.0 | 3.0 | 2.0 |
| Heterogeneity | 5.5 | 4.33 | 2.0 | 2.0 | 2.0 |
| Measurement Error | 13.0 | 0.00 | 2.0 | 3.0 | 2.0 |
| Noise Type | 11.5 | 1.50 | 2.0 | 4.0 | 2.0 |
| Missing Data | 13.0 | 0.00 | 2.0 | 3.0 | 2.0 |

• Analysis

  – Overall mean ranking across 5 scenarios: 10.07
  – Average standard deviation: 1.77

• Key Observations
  – Moderately stable performance across scenarios
  – Best performance in Heterogeneity scenario (rank 5.5)

*Note: Rankings are relative positions among all tested algorithms, while level scores are absolute quantitized measures of capability.*


────────────────────────────────────────────────────────
Benchmarking Results
────────────────────────────────────────────────────────

• Comparative Performance
  – The benchmarking compared 19 different causal discovery algorithms across multiple scenarios.
  – Each algorithm was evaluated on performance (accuracy), efficiency (runtime), and composite metrics.
  – Composite scores combine performance and efficiency (weights: 0.8 and 0.2 respectively).
  – Metrics are scored from 1-5, with 5 being the best.

• Algorithm Rankings

| Scenario | Mean Rank | Std Dev | Performance | Efficiency | Composite |
|----------|-----------|---------|-------------|------------|------------|
| Scalability | 7.4 | 3.00 | 2.0 | 3.0 | 2.0 |
| Heterogeneity | 5.5 | 4.33 | 2.0 | 2.0 | 2.0 |
| Measurement Error | 13.0 | 0.00 | 2.0 | 3.0 | 2.0 |
| Noise Type | 11.5 | 1.50 | 2.0 | 4.0 | 2.0 |
| Missing Data | 13.0 | 0.00 | 2.0 | 3.0 | 2.0 |

• Analysis

  – Overall mean ranking across 5 scenarios: 10.07
  – Average standard deviation: 1.77

• Key Observations
  – Moderately stable performance across scenarios
  – Best performance in Heterogeneity scenario (rank 5.5)

*Note: Rankings are relative positions among all tested algorithms, while level scores are absolute quantitized measures of capability.*


# FCI

Below is a comprehensive profile of the Fast Causal Inference (FCI) algorithm, structured according to the seven “degrees” (dimensions) of analysis. This profile synthesizes:  
• File #1 (the hyperparameter settings),  
• File #2 (benchmarking results),  
• External knowledge about FCI (including peer-reviewed papers, publicly available documentation, and community discussions),  
• General principles of causal discovery.  

────────────────────────────────────────────────────────────────────
1. Hyper-Parameters Sensitivity
────────────────────────────────────────────────────────────────────

• Number of Key Hyperparameters  
  – FCI has three principal hyperparameters (based on File #1):  
    1) α (significance level),  
    2) The choice of independence test (indep_test),  
    3) The depth parameter controlling how exhaustively conditional independence tests are performed.  

• Tuning Difficulty  
  – α (significance level) is among the most impactful parameters. Its default is often 0.05, though File #1 suggests using 0.1 for very small samples (< 500), 0.05 for moderate-sized samples (500 to 10,000), and 0.01 for extremely large samples (> 10,000).  
  – The independence test parameter (indep_test) depends on data type and modeling assumptions. Commonly used options in practice include:  
     “fisherz” for continuous data under a Gaussian assumption,  
     “chisq” for discrete data,  
     “gsq,” “kci,” or “rcit” for more general or non-linear data.  
  – Depth settings can be unrestricted (-1) or limited to reduce computation. While the defaults are straightforward (e.g., unlimited depth for small graphs), tuning ultimately depends on computational constraints and how dense the graph might be.  

• Sensitivity  
  – α: Small changes (e.g., from 0.05 to 0.01) can sharply reduce false positives but potentially increase false negatives. Benchmarks in File #2 indicate that performance metrics (e.g., adjacency precision) shift when α is made more conservative.  
  – Depth: Restricting depth can speed up the skeleton discovery step considerably but risks missing some indirect connections, as fewer conditional sets are tested.  

• Critique/Extension  
  – Parameters that control search complexity (e.g., depth) heavily influence runtime, particularly on large or dense graphs.  
  – Parameters controlling statistical tests (e.g., α and the independence test selection) most directly affect the quality of the discovered causal structure (false positives / false negatives in edges).

────────────────────────────────────────────────────────────────────
2. Robustness & Scalability
────────────────────────────────────────────────────────────────────

• Tolerance to Bad Data Quality  
  – Missing Data: FCI can tolerate missing data if the chosen independence test supports it (e.g., some tests impute or ignore missing cases). Benchmarks in File #2 show FCI performing around a moderate level (neither the best nor the worst) when data are missing.  
  – Measurement/Observation Error: FCI’s performance decreases with severe noise but remains relatively stable for moderate noise levels. Its ability to discover latent structures can mitigate some confounding effects of measurement error, though the precision of edges can drop if noise is extreme.  

• Tolerance to Sparse/Dense Connected Systems  
  – Sparse Networks: FCI typically performs well in sparse graphs, because fewer edges mean fewer complex conditional independence tests.  
  – Dense Networks: FCI can handle denser connectivity but at notably increased computational cost, as more conditional tests are required and orienting edges becomes more complex.  

• Scalability  
  – As the number of variables grows, the number of required conditional independence tests can explode. File #2 reports an above-average computational burden for FCI once the variable count is large.  
  – FCI can still be applied to moderately high-dimensional data, but runtime may become prohibitive without restricting the depth parameter or using parallelization strategies.  

• Critique/Extension  
  – Parallel Implementation: Some implementations offer parallelized independence testing to help with large datasets.  
  – Approximate or Bounded Depth: Restricting the maximum depth of search is a common strategy to reduce runtime (though this may sacrifice some accuracy).

────────────────────────────────────────────────────────────────────
3. Mixed Data & Complex Functions
────────────────────────────────────────────────────────────────────

• Noise Type  
  – FCI itself does not strictly require Gaussian noise; it simply needs an appropriate conditional independence test (e.g., “kci” or “rcit” from File #1) to detect non-linear, non-Gaussian dependencies.  

• Mixed Data (Continuous & Discrete)  
  – Users can choose independence tests suited to mixed data (e.g., “gsq”) or generalized tests that handle a variety of variable types. According to File #1, “kci,” “fastkci,” and “rcit” may also address nonlinear interactions across data types.  

• Heterogeneous Data  
  – File #2 suggests moderate performance under heterogeneous conditions (e.g., data collected from different sources or distributions). FCI’s separation-of-independences approach can still work if independence tests remain valid across distributions.  

• Complex Functional Forms  
  – Non-linear relationships are detectable if the chosen independence test can pick them up. FCI, however, does not automatically model these relationships; it relies on test outcomes.  

• Critique/Extension  
  – The primary limitation is reliance on an accurate independence test. If the test poorly captures non-linear or complex interactions, FCI may miss true edges or introduce extraneous ones.  
  – Some advanced versions of FCI or user-developed variations incorporate kernel-based tests, enabling more robust detection of complicated functional relationships.

────────────────────────────────────────────────────────────────────
4. Computational Complexity
────────────────────────────────────────────────────────────────────

• Theoretical Time Complexity  
  – FCI is often cited as having an exponential worst-case time complexity with respect to the number of variables. A simplified expression is presented here: <temp>[O(2^n)]</temp>.  
  – In practice, the real cost is heavily influenced by how many adjacency constraints it identifies early in the process.

• Variability in Practical Usage  
  – Depth-limited FCI can reduce runtime to something more manageable (potentially polynomial in many real-world scenarios).  
  – Higher significance levels (larger α) may reduce the total number of conditional independence tests because edges are removed more slowly, but they risk more spurious edges that must be tested and oriented later.  

• Critique/Extension  
  – The worst-case complexity is rarely reached in sparse, real-world networks.  
  – Parallel computing and caching of test results (mentioned in community forums) can improve runtime substantially.

────────────────────────────────────────────────────────────────────
5. Interpretability
────────────────────────────────────────────────────────────────────

• Output Format  
  – FCI outputs a Partial Ancestral Graph (PAG), which represents causal constraints among variables, including potential latent confounders.  

• Strength of the Output Format  
  – PAGs explicitly encode uncertain edges (e.g., bidirected edges indicating hidden confounders, circle endpoints indicating ambiguous directions). This offers rich information about causal possibilities.  

• Limitations of the Output Format  
  – PAGs can be less intuitive than DAGs or CPDAGs. Edges may remain unoriented or partially oriented, especially if data alone cannot resolve their direction.  
  – Users often require additional domain knowledge to interpret or further refine certain ambiguous edges.  

• Critique/Extension  
  – Researchers recommend domain experts consult the PAG’s edge marks to hypothesize plausible latent variables or refine uncertain orientations.  
  – Visualization libraries for FCI exist but require a solid grasp of the edge nomenclature.

────────────────────────────────────────────────────────────────────
6. Assumptions
────────────────────────────────────────────────────────────────────

• Critical Assumptions  
  – Markov and Faithfulness: The distribution of the data must align with the “true” causal graph’s conditional independencies, without pathological cancellations.  
  – No Cycles: FCI assumes acyclicity among observed variables, even though it allows for the possibility of latent confounders.  
  – Causal Sufficiency is relaxed compared to simpler algorithms (like PC), meaning FCI can account for possible hidden variables.  

• Violation Impact  
  – Violating faithfulness can produce incomplete or misleading PAGs (e.g., missing edges or unvalued circle endpoints).  
  – When hidden confounding is extremely strong and the sample size is low, the discovered PAG can have many ambiguities.  

• Critique/Extension  
  – FCI is often chosen because it explicitly relaxes causal sufficiency assumptions. Indeed, that is a main strength over simpler methods.  
  – Partial violations of faithfulness can degrade accuracy but do not necessarily invalidate the overall skeleton of the PAG.

────────────────────────────────────────────────────────────────────
7. Real-World Benchmarks
────────────────────────────────────────────────────────────────────

• Performance on Real Datasets  
  – File #2 indicates that FCI often performs at a middle or upper-middle rank across different metrics (e.g., adjacency precision, sensitivity to hidden variables).  
  – Empirical studies (from external references) report that FCI often outperforms algorithms that assume no latent confounders when hidden variables or selection bias are indeed present.

• Practical Tips  
  – Adjust α based on sample size: Larger samples can justify smaller α for fewer false positives; smaller samples may require a higher α to avoid missing genuine edges.  
  – Parallelizing the conditional independence tests or limiting depth can make FCI more tractable for large-dimensional datasets.  
  – Domain knowledge plays a crucial role in resolving uncertain orientations in the PAG, ensuring the final model is more interpretable and actionable.  

• Common Pitfalls  
  – Using an independence test misaligned with the data’s underlying distribution (e.g., only using Fisher’s Z in the presence of non-linear or discrete variables) can lead to incorrect edges.  
  – Overly restrictive significance levels with small sample sizes can cause many missed edges, while overly lenient significance levels can flood the output with spurious edges.

────────────────────────────────────────────────────────────────────
Summary
────────────────────────────────────────────────────────────────────
FCI (Fast Causal Inference) is a widely respected causal discovery algorithm that relaxes the assumption of no hidden confounders, making it especially valuable when latent variables or selection bias may be present. Its key hyperparameters—significance level (α), independence test choice, and depth constraint—all substantially affect how many edges are retained and how computationally heavy the procedure becomes. Although FCI can handle both continuous and discrete data (and even mixed types) via different independence tests, its runtime grows quickly with the number of variables and the complexity of the network.

Because FCI outputs a Partial Ancestral Graph (PAG), it can reveal potential latent confounding, but this representation may contain uncertain or bidirected edges requiring careful interpretation. This complexity can also make it challenging to tune parameters optimally without domain or statistical expertise. When properly configured, FCI has proven effective in real-world applications like genetics, climate science, and social networks—domains where hidden confounders often lurk. Overall, its flexibility and ability to account for unobserved variables are balanced by higher computational demands and some interpretability challenges relative to simpler methods.────────────────────────────────────────────────────────
Benchmarking Results
────────────────────────────────────────────────────────

• Comparative Performance
  – The benchmarking compared 19 different causal discovery algorithms across multiple scenarios.
  – Each algorithm was evaluated on performance (accuracy), efficiency (runtime), and composite metrics.
  – Composite scores combine performance and efficiency (weights: 0.8 and 0.2 respectively).
  – Metrics are scored from 1-5, with 5 being the best.

• Algorithm Rankings

| Scenario | Mean Rank | Std Dev | Performance | Efficiency | Composite |
|----------|-----------|---------|-------------|------------|------------|
| Scalability | 6.0 | 2.19 | 3.0 | 5.0 | 4.0 |
| Heterogeneity | 10.0 | 2.00 | 4.0 | 4.0 | 4.0 |
| Measurement Error | 7.0 | 0.00 | 4.0 | 5.0 | 4.0 |
| Noise Type | 7.0 | 0.00 | 3.0 | 5.0 | 4.0 |
| Missing Data | 7.0 | 0.00 | 4.0 | 5.0 | 4.0 |

• Analysis

  – Overall mean ranking across 5 scenarios: 7.40
  – Average standard deviation: 0.84

• Key Observations
  – Very stable performance across scenarios (low variance)
  – Best performance in Scalability scenario (rank 6.0)

*Note: Rankings are relative positions among all tested algorithms, while level scores are absolute quantitized measures of capability.*


────────────────────────────────────────────────────────
Benchmarking Results
────────────────────────────────────────────────────────

• Comparative Performance
  – The benchmarking compared 19 different causal discovery algorithms across multiple scenarios.
  – Each algorithm was evaluated on performance (accuracy), efficiency (runtime), and composite metrics.
  – Composite scores combine performance and efficiency (weights: 0.8 and 0.2 respectively).
  – Metrics are scored from 1-5, with 5 being the best.

• Algorithm Rankings

| Scenario | Mean Rank | Std Dev | Performance | Efficiency | Composite |
|----------|-----------|---------|-------------|------------|------------|
| Scalability | 6.0 | 2.19 | 3.0 | 5.0 | 4.0 |
| Heterogeneity | 10.0 | 2.00 | 4.0 | 4.0 | 4.0 |
| Measurement Error | 7.0 | 0.00 | 4.0 | 5.0 | 4.0 |
| Noise Type | 7.0 | 0.00 | 3.0 | 5.0 | 4.0 |
| Missing Data | 7.0 | 0.00 | 4.0 | 5.0 | 4.0 |

• Analysis

  – Overall mean ranking across 5 scenarios: 7.40
  – Average standard deviation: 0.84

• Key Observations
  – Very stable performance across scenarios (low variance)
  – Best performance in Scalability scenario (rank 6.0)

*Note: Rankings are relative positions among all tested algorithms, while level scores are absolute quantitized measures of capability.*


────────────────────────────────────────────────────────
Benchmarking Results
────────────────────────────────────────────────────────

• Comparative Performance
  – The benchmarking compared 19 different causal discovery algorithms across multiple scenarios.
  – Each algorithm was evaluated on performance (accuracy), efficiency (runtime), and composite metrics.
  – Composite scores combine performance and efficiency (weights: 0.8 and 0.2 respectively).
  – Metrics are scored from 1-5, with 5 being the best.

• Algorithm Rankings

| Scenario | Mean Rank | Std Dev | Performance | Efficiency | Composite |
|----------|-----------|---------|-------------|------------|------------|
| Scalability | 6.0 | 2.19 | 3.0 | 5.0 | 4.0 |
| Heterogeneity | 10.0 | 2.00 | 4.0 | 4.0 | 4.0 |
| Measurement Error | 7.0 | 0.00 | 4.0 | 5.0 | 4.0 |
| Noise Type | 7.0 | 0.00 | 3.0 | 5.0 | 4.0 |
| Missing Data | 7.0 | 0.00 | 4.0 | 5.0 | 4.0 |

• Analysis

  – Overall mean ranking across 5 scenarios: 7.40
  – Average standard deviation: 0.84

• Key Observations
  – Very stable performance across scenarios (low variance)
  – Best performance in Scalability scenario (rank 6.0)

*Note: Rankings are relative positions among all tested algorithms, while level scores are absolute quantitized measures of capability.*


# GES

Below is a comprehensive profile of the Greedy Equivalence Search (GES) algorithm, organized according to the seven degrees (dimensions) for causal discovery algorithm profiling. This synthesis draws upon the provided hyperparameter settings, benchmarking outcomes, additional references from external knowledge sources, as well as general domain knowledge of GES.

────────────────────────────────────────────────────────────────────────
1. HYPER-PARAMETERS SENSITIVITY
────────────────────────────────────────────────────────────────────────

• Number of Key Hyperparameters  
  GES has two overarching hyperparameters of interest:  
  1) The “score_func” (scoring function)  
  2) The “maxP” parameter (maximum number of parents allowed per node)  

  The scoring function determines how each candidate structure is evaluated (e.g., BIC, BDeu, or more sophisticated non-parametric scores).  
  The “maxP” parameter constrains the potential number of incoming edges per node, directly influencing the search space.

• Tuning Difficulty  
  – Scoring Function:  
    Default “local_score_BIC” works well for linear or near-linear data, balancing model fit and complexity. For discrete data, “local_score_BDeu” can be more suitable, while “marginal_general” or “marginal_multi” are recommended for complex, potentially non-linear relationships (albeit at a higher computational cost).  
  – maxP:  
    The suggested defaults vary by graph size (e.g., maxP=3, 5, or 7) to keep complexity manageable. For very small networks (<10 nodes), one can leave this parameter unlimited.  

  Because these defaults are relatively straightforward, domain experts or automated routines (including large language models) often have little difficulty choosing sensible starting points.

• Sensitivity  
  – Small changes in “score_func” can noticeably alter the discovered graph: a parametric scoring function (BIC) may favor sparser structures for continuous data, while a non-parametric choice (e.g., marginal_general) may detect more nuanced dependencies but require more runtime.  
  – Adjusting “maxP” even by one parent can substantially affect computational cost in larger graphs because it expands or contracts the space of candidate edges.

• Critique/Extension  
  – Graph-Search vs. Statistical Score Parameters:  
    The “maxP” parameter predominantly controls search complexity, with each additional allowable parent expanding the search exponentially. Meanwhile, the scoring function selection has greater impact on how edges are penalized or rewarded statistically, thus affecting model fit and orientation decisions.

────────────────────────────────────────────────────────────────────────
2. ROBUSTNESS & SCALABILITY
────────────────────────────────────────────────────────────────────────

• Tolerance to Bad Data Quality  
  – Missing Data:  
    Benchmarks suggest GES remains robust in many missing-data scenarios, especially if the chosen scoring function can accommodate incomplete observations. Some performance degradation is inevitable with severe missingness, but the algorithm’s composite rank for handling missing data is among the better-performing methods (based on the benchmark “levels” of performance and efficiency).  
  – Measurement/Observation Error:  
    GES’s ranking and levels for measurement error also indicate that it can maintain good performance and efficiency under moderate noise. Significant error can degrade the correctness of edge orientations, but the method still retains reasonable reliability relative to other techniques.

• Tolerance to Sparse vs. Dense Networks  
  – Sparse Networks:  
    GES tends to discover sparse structures effectively, particularly when using penalized scores such as BIC.  
  – Moderately Dense Networks:  
    It can still adapt to more connected configurations, although the search space grows quickly; using “maxP” can help curb exponential blow-ups.

• Scalability  
  – With moderate numbers of variables, GES is relatively efficient. Benchmark data did indicate that its efficiency ranking drops for high-dimensional settings, suggesting that runtime can become a bottleneck if the graph is large or maxP is set too high.  
  – In practice, many users turn to variations like Fast GES (FGES), which parallelizes or approximates certain edges to maintain scalability.

• Critique/Extension  
  – Parallelization and Approximation: FGES exploits parallel computation and more efficient data structures, providing a faster alternative in large-sample or high-dimensional contexts without significantly hurting performance accuracy.

────────────────────────────────────────────────────────────────────────
3. MIXED DATA & COMPLEX FUNCTIONS
────────────────────────────────────────────────────────────────────────

• Noise Type  
  – GES can handle both Gaussian and non-Gaussian noise, depending on the selected scoring function. For instance, BIC-based local scores are typically aligned with (approximately) Gaussian error assumptions for continuous variables, whereas BDeu is more suitable for purely discrete variables.

• Mixed Data (Continuous & Discrete)  
  – With appropriate “score_func,” GES can incorporate both continuous and discrete variables in the same model. In the provided hyperparameters, “local_score_marginal_general” and “local_score_marginal_multi” are specifically mentioned for more complex data mixtures or non-linear relationships.

• Heterogeneous Data  
  – Benchmarks labeled “Heterogeneity” show strong combined performance and efficiency (high “composite” rating). This suggests GES can remain effective even when data come from slightly different underlying distributions, provided the user carefully configures the scoring function.

• Complex Functional Forms  
  – Out-of-the-box, GES often assumes linear or log-linear forms, especially when using BIC. However, non-parametric scoring options (marginal scores) allow detection of more complex forms at the expense of notably higher runtime.

• Critique/Extension  
  – Users looking for robust non-linear discovery tend to pair GES with advanced scoring metrics or domain knowledge constraints, as purely linear approaches may miss subtler non-linear dependencies.

────────────────────────────────────────────────────────────────────────
4. COMPUTATIONAL COMPLEXITY
────────────────────────────────────────────────────────────────────────

• Theoretical Time Complexity  
  – The typical worst-case complexity for GES is <temp>[O(n^4)]</temp>, though actual runtime can be lower in practical scenarios or higher if “maxP” is large.

• Variability in Practical Usage  
  – Tighter “maxP” constraints can reduce the search space dramatically, especially in networks with many potential edges.  
  – More complex, non-parametric score functions (“marginal_general” / “marginal_multi”) lengthen computation time and can be a limiting factor in large datasets.

• Critique/Extension  
  – Worst-case vs. Typical Performance:  
    In the worst-case, GES enumerates many potential edge additions and deletions, but typical performance can be far better in sparse or moderately sized problems.  
  – Hardware Considerations:  
    Parallel implementations (FGES) provide significant runtime relief, making large-scale analysis more feasible when multi-core processing is available.

────────────────────────────────────────────────────────────────────────
5. INTERPRETABILITY
────────────────────────────────────────────────────────────────────────

• Output Format  
  – GES returns a Completed Partially Directed Acyclic Graph (CPDAG), reflecting the set of DAGs that share the same likelihood score and are statistically indistinguishable given the data.

• Strength of the Output Format  
  – CPDAGs convey which edges are definitely oriented vs. which remain undetermined by the data. This can be highly interpretable, showing robustly supported causal directions versus potential bidirectional ambiguities.  

• Limitations of the Output Format  
  – Unoriented edges in the CPDAG can be common, particularly when the data are insufficient to break certain equivalences.  
  – Standard GES implementations typically do not provide confidence intervals or p-values for edges in the final graph.

• Critique/Extension  
  – Domain experts often incorporate additional knowledge or run supplementary analyses (e.g., constraint-based checks) to further orient uncertain edges.  
  – Post-processing, such as resampling or bootstrapping, can give approximate confidence scores for edges if desired, though it is not native in basic GES implementations.

────────────────────────────────────────────────────────────────────────
6. ASSUMPTIONS
────────────────────────────────────────────────────────────────────────

• Critical Assumptions  
  1) Causal Markov and Faithfulness: The observed distribution is assumed to reflect the true causal DAG structure without degeneracies.  
  2) Causal Sufficiency: No hidden confounders or unmeasured variables strongly affecting the relationships among measured variables.  
  3) Acyclicity: The true structure must be a directed acyclic graph.

• Violation Impact  
  – If hidden common causes exist, GES can place erroneous edges or fail to discover genuine causal pathways.  
  – If faithfulness is violated, subtle dependencies might go undetected or incorrectly oriented.

• Critique/Extension  
  – Some variants relax causal sufficiency by explicitly modeling latent variables, but standard GES assumes all relevant causes are observed.  
  – Minor assumption violations typically produce moderate distortions in orientation or adjacency; more severe violations (like strong confounding) can introduce critical errors.

────────────────────────────────────────────────────────────────────────
7. (OPTIONAL) REAL-WORLD BENCHMARKS
────────────────────────────────────────────────────────────────────────

• Performance on Real Datasets  
  – GES frequently performs among the more competitive methods on real-world tasks, especially when the data set is sizable and well-sampled.  
  – It can achieve high adjacency precision in domains like genomics (gene regulatory networks) or neuroscience (brain connectivity analysis) where it has been widely tested.

• Practical Tips  
  – Choosing a score function well-suited to the data type substantially improves performance.  
  – Limiting “maxP” can prevent excessive runtimes in moderately sized or large graphs.  
  – Domain knowledge is often used after GES identifies a CPDAG, helping to resolve undirected edges and validate spurious connections.

• Known Pitfalls  
  – If the data are extremely sparse, or if strong violations of causal sufficiency exist, GES may orient edges incorrectly.  
  – In highly non-linear settings, purely parametric scoring functions (like BIC) can miss important relationships, so more advanced scoring methods or combined approaches are recommended.

────────────────────────────────────────────────────────────────────────
CONCLUSION
────────────────────────────────────────────────────────────────────────
In summary, GES is a well-established score-based causal discovery algorithm known for its relatively balanced performance on both sparse and somewhat dense networks. It offers flexible scoring options (BIC, BDeu, non-parametric variants) and a “maxP” hyperparameter to manage complexity. Although its runtime may grow in large or complex systems, variants such as Fast GES (FGES) and careful limiting of the search space help mitigate scalability concerns. The CPDAG output is highly interpretable, though ambiguous edges often require further refinement or domain expertise. As with many causal discovery approaches, GES assumes acyclicity, sufficiency, and faithfulness; significant departures from these assumptions can degrade results. Overall, it remains a widely used, adaptable method for causal structure learning across diverse fields.────────────────────────────────────────────────────────
Benchmarking Results
────────────────────────────────────────────────────────

• Comparative Performance
  – The benchmarking compared 19 different causal discovery algorithms across multiple scenarios.
  – Each algorithm was evaluated on performance (accuracy), efficiency (runtime), and composite metrics.
  – Composite scores combine performance and efficiency (weights: 0.8 and 0.2 respectively).
  – Metrics are scored from 1-5, with 5 being the best.

• Algorithm Rankings

| Scenario | Mean Rank | Std Dev | Performance | Efficiency | Composite |
|----------|-----------|---------|-------------|------------|------------|
| Scalability | 8.5 | 4.03 | 4.0 | 2.0 | 4.0 |
| Heterogeneity | 3.0 | 0.00 | 5.0 | 5.0 | 5.0 |
| Measurement Error | 3.0 | 0.00 | 5.0 | 5.0 | 5.0 |
| Noise Type | 3.0 | 0.00 | 4.0 | 4.0 | 5.0 |
| Missing Data | 3.0 | 0.00 | 5.0 | 5.0 | 5.0 |

• Analysis

  – Overall mean ranking across 5 scenarios: 4.10
  – Average standard deviation: 0.81

• Key Observations
  – Very stable performance across scenarios (low variance)
  – Best performance in Heterogeneity scenario (rank 3.0)

*Note: Rankings are relative positions among all tested algorithms, while level scores are absolute quantitized measures of capability.*


────────────────────────────────────────────────────────
Benchmarking Results
────────────────────────────────────────────────────────

• Comparative Performance
  – The benchmarking compared 19 different causal discovery algorithms across multiple scenarios.
  – Each algorithm was evaluated on performance (accuracy), efficiency (runtime), and composite metrics.
  – Composite scores combine performance and efficiency (weights: 0.8 and 0.2 respectively).
  – Metrics are scored from 1-5, with 5 being the best.

• Algorithm Rankings

| Scenario | Mean Rank | Std Dev | Performance | Efficiency | Composite |
|----------|-----------|---------|-------------|------------|------------|
| Scalability | 8.5 | 4.03 | 4.0 | 2.0 | 4.0 |
| Heterogeneity | 3.0 | 0.00 | 5.0 | 5.0 | 5.0 |
| Measurement Error | 3.0 | 0.00 | 5.0 | 5.0 | 5.0 |
| Noise Type | 3.0 | 0.00 | 4.0 | 4.0 | 5.0 |
| Missing Data | 3.0 | 0.00 | 5.0 | 5.0 | 5.0 |

• Analysis

  – Overall mean ranking across 5 scenarios: 4.10
  – Average standard deviation: 0.81

• Key Observations
  – Very stable performance across scenarios (low variance)
  – Best performance in Heterogeneity scenario (rank 3.0)

*Note: Rankings are relative positions among all tested algorithms, while level scores are absolute quantitized measures of capability.*


────────────────────────────────────────────────────────
Benchmarking Results
────────────────────────────────────────────────────────

• Comparative Performance
  – The benchmarking compared 19 different causal discovery algorithms across multiple scenarios.
  – Each algorithm was evaluated on performance (accuracy), efficiency (runtime), and composite metrics.
  – Composite scores combine performance and efficiency (weights: 0.8 and 0.2 respectively).
  – Metrics are scored from 1-5, with 5 being the best.

• Algorithm Rankings

| Scenario | Mean Rank | Std Dev | Performance | Efficiency | Composite |
|----------|-----------|---------|-------------|------------|------------|
| Scalability | 8.5 | 4.03 | 4.0 | 2.0 | 4.0 |
| Heterogeneity | 3.0 | 0.00 | 5.0 | 5.0 | 5.0 |
| Measurement Error | 3.0 | 0.00 | 5.0 | 5.0 | 5.0 |
| Noise Type | 3.0 | 0.00 | 4.0 | 4.0 | 5.0 |
| Missing Data | 3.0 | 0.00 | 5.0 | 5.0 | 5.0 |

• Analysis

  – Overall mean ranking across 5 scenarios: 4.10
  – Average standard deviation: 0.81

• Key Observations
  – Very stable performance across scenarios (low variance)
  – Best performance in Heterogeneity scenario (rank 3.0)

*Note: Rankings are relative positions among all tested algorithms, while level scores are absolute quantitized measures of capability.*




===============================================

Think in-depth and thoroughly step by step. Please include the reasoning process, the ultimate reason why the picked algorithm beats the others and finally the selected algorithm in the JSON format. Do not return any other text or comments:

{
  "reasoning": "reasoning process",
  "reason": "ultimate reason why it beats the others"
  "algorithm": "algorithm_name",
}
