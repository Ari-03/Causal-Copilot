You will conduct causal discovery on the Tabular Dataset ./demo_data/20250327_115101/house_price/house_price.csv containing the following Columns:

Id	MSSubClass	MSZoning	LotFrontage	LotArea	Street	LotShape	LandContour	Utilities	LotConfig	LandSlope	Neighborhood	Condition1	Condition2	BldgType	HouseStyle	OverallQual	OverallCond	YearRemodAdd	RoofStyle	RoofMatl	Exterior1st	Exterior2nd	MasVnrArea	ExterQual	ExterCond	Foundation	BsmtQual	BsmtCond	BsmtExposure	BsmtFinType1	BsmtFinSF1	BsmtFinType2	BsmtFinSF2	BsmtUnfSF	TotalBsmtSF	Heating	HeatingQC	CentralAir	Electrical	1stFlrSF	2ndFlrSF	LowQualFinSF	GrLivArea	BsmtFullBath	BsmtHalfBath	FullBath	HalfBath	BedroomAbvGr	KitchenAbvGr	KitchenQual	TotRmsAbvGrd	Functional	Fireplaces	FireplaceQu	GarageType	GarageYrBlt	GarageFinish	GarageCars	GarageArea	GarageQual	GarageCond	PavedDrive	WoodDeckSF	OpenPorchSF	EnclosedPorch	3SsnPorch	ScreenPorch	PoolArea	MiscVal	MoSold	YrSold	SaleType	SaleCondition	SalePrice	domain_index

The Detailed Background Information is listed below:

['The variable names in your dataset appear to be quite meaningful and provide insights into their respective characteristics of house prices. Below, I provide detailed explanations of each variable, possible causal relationships, and background domain knowledge that could assist in designing causal discovery algorithms.\n\n### 1. Detailed Explanation about the Variables\n\n- **PoolQC**: Quality of the pool (categorical: Excellent, Good, Average, Fair, or None).\n- **BsmtFinSF1**: Type 1 finished square feet of the basement.\n- **GarageArea**: Size of the garage in square feet.\n- **Street**: Type of road access (categorical: Paved or Gravel).\n- **ScreenPorch**: Area of the screened porch in square feet.\n- **3SsnPorch**: Area of the three-season porch in square feet.\n- **MSSubClass**: Identifies the type of dwelling (numerical).\n- **FullBath**: Number of full bathrooms above grade.\n- **1stFlrSF**: Square feet of the first floor.\n- **WoodDeckSF**: Area of the wood deck in square feet.\n- **BsmtFinSF2**: Type 2 finished square feet of basement.\n- **LotShape**: General shape of the property (categorical: Regular, IR1, IR2, and IR3).\n- **PavedDrive**: Indicates if the driveway is paved (categorical: Y, P, N).\n- **Exterior1st**: Exterior covering on the house (categorical).\n- **BedroomAbvGr**: Number of bedrooms above ground level.\n- **OverallCond**: Condition of the house (numerically rated).\n- **LandSlope**: Slope of the property (categorical: Gtl, Mod, Sev).\n- **KitchenAbvGr**: Number of kitchens above ground level.\n- **OverallQual**: Overall material and finish quality (numerically rated).\n- **GarageCars**: Size of the garage in terms of car capacity.\n- **2ndFlrSF**: Square feet of the second floor.\n- **GarageType**: Garage location (categorical).\n- **YrSold**: Year the house was sold.\n- **MiscVal**: Miscellaneous value (numerically).\n- **Foundation**: Type of foundation (categorical).\n- **RoofMatl**: Primary material of the roof (categorical).\n- **GarageFinish**: Interior finish of the garage (categorical).\n- **LotFrontage**: Linear feet of the lot facing the street.\n- **YearRemodAdd**: Year when the house was remodeled.\n- **GrLivArea**: Above grade living area in square feet.\n- **GarageCond**: Condition of the garage (categorical).\n- **BsmtExposure**: Exposure of the basement (categorical).\n- **Electrical**: Electrical system (categorical).\n- **BsmtFullBath**: Number of full bathrooms in the basement.\n- **TotRmsAbvGrd**: Total number of rooms above ground level.\n- **LowQualFinSF**: Low quality finished square feet.\n- **BsmtHalfBath**: Number of half bathrooms in the basement.\n- **KitchenQual**: Kitchen quality (categorical).\n- **BsmtCond**: Condition of the basement (categorical).\n- **BldgType**: Type of dwelling (categorical).\n- **SaleCondition**: Condition of sale (categorical).\n- **HouseStyle**: Style of the house (categorical).\n- **EnclosedPorch**: Area of the enclosed porch in square feet.\n- **Heating**: Type of heating (categorical).\n- **ExterCond**: Condition of the exterior (categorical).\n- **Exterior2nd**: Second exterior material (categorical).\n- **Functional**: Home functionality classification (categorical).\n- **YearBuilt**: Original construction year of the house.\n- **LotArea**: Lot size in square feet.\n- **SalePrice**: Sale price of the house (dependent variable).\n- **GarageQual**: Garage quality (categorical).\n- **FireplaceQu**: Fireplace quality (categorical).\n- **RoofStyle**: Type of roof (categorical).\n- **MSZoning**: General zoning classification (categorical).\n- **Id**: Unique identifier for each house.\n- **PoolArea**: Area of the pool.\n- **Fence**: Fence quality (categorical).\n- **TotalBsmtSF**: Total square feet of the basement.\n- **Neighborhood**: Physical locations within the city (categorical).\n- **HalfBath**: Number of half baths above ground.\n- **MoSold**: Month the house was sold (numerical).\n- **Condition1**: Proximity to various conditions (categorical).\n- **HeatingQC**: Heating quality and condition (categorical).\n- **Utilities**: Type of utilities available (categorical).\n- **CentralAir**: Central air conditioning status (Y, N).\n- **MiscFeature**: Miscellaneous feature (categorical).\n- **LandContour**: Flatness of the property (categorical).\n- **BsmtFinType1**: Type of finished area in the basement (categorical).\n- **GarageYrBlt**: Year the garage was built.\n- **BsmtQual**: Quality of the basement (categorical).\n- **MasVnrArea**: Masonry veneer area in square feet.\n- **Alley**: Type of alley access (categorical).\n- **Condition2**: Proximity to various conditions (categorical).\n- **SaleType**: Type of sale (categorical).\n- **Fireplaces**: Number of fireplaces.\n- **BsmtFinType2**: Second type of finished area in the basement (categorical).\n- **LotConfig**: Lot configuration (categorical).\n- **OpenPorchSF**: Area of the open porch in square feet.\n- **BsmtUnfSF**: Unfinished square feet of the basement.\n- **ExterQual**: Exterior quality of the home (categorical).\n\n### 2. Possible Causal Relations among These Variables\n\n- **SalePrice** is likely influenced by multiple factors:\n  - **OverallQual**, **GrLivArea**, **GarageCars**, and **FullBath** could directly affect **SalePrice** as they relate to size and quality.\n  - **Neighborhood** impacts **SalePrice** due to desirability.\n  - **YearBuilt** and **YearRemodAdd** would affect the **SalePrice**, reflecting the age and modernity of the house.\n  - **GarageArea** and features like **PoolArea** may contribute to the perceived value.\n  - Variables like **LotArea**, **LotFrontage**, and **LandSlope** could influence the land value aspect of **SalePrice**.\n  \n- **Types of features** (e.g., **Exterior1st**, **GarageType**, **KitchenQual**) can influence buyer preferences and therefore **SalePrice**.\n  \n- **OverallCond** and **ExterCond** may moderate the effects of size and quality variables, showing that houses in good condition may sell for more regardless of size.\n  \n- Variables like **MoSold** can indicate seasonal influences on **SalePrice**, where homes could sell for higher prices during certain times of the year.\n\n### 3. Other Background Domain Knowledge\n\n- **Real Estate Trends**: Market trends can affect housing prices significantly. Factors such as economic conditions, interest rates, and local real estate demand should be considered when analyzing relationships in the dataset.\n\n- **Local Zoning Laws**: Understanding local zoning regulations (reflected in **MSZoning**) can clarify how property modifications may be legally feasible and thus can impact property values.\n\n- **Construction Materials**: Knowledge about construction materials and their impact on property durability, thermal efficiency, and visual appeal can be useful for interpreting the impacts of attributes like **RoofMatl** and **Exterior1st**.\n\n- **Demographic Factors**: Socio-economic factors in neighborhoods can also play roles in determining house prices based on buyer preferences influenced by social status, community amenities, schools, and safety.\n\n- **Environmental Considerations**: Factors such as flood risk, proximity to parks, and overall neighborhood safety (reflected in variables like **Alley** and **Condition2**) can also influence price.\n\nIn summary, these variables present significant insights and potential causal pathways relevant to housing price determination. Understanding these factors combined with domain knowledge will facilitate effective causal discovery analyses.']

The Statistics Information about the dataset is:

The dataset has the following characteristics:

Data Type: The overall data type is Mixture.

The sample size is 1460 with 75 features. This dataset is not time-series data. Data Quality: There are missing values in the dataset.

Statistical Properties:
- Linearity: The relationships between variables are not predominantly linear.
- Gaussian Errors: The errors in the data do not follow a Gaussian distribution.
- Heterogeneity: The dataset is heterogeneous. 

- Domain Index: YearBuilt

Based on the above information, please select the best-suited algorithm from the following candidate (the order of the algorithm candidates is not important):

dict_keys(['CDNOD', 'IAMBnPC', 'PC', 'FCI', 'GRaSP'])

===============================================
Note that the user can only wait for 1440.0 minutes for the algorithm execution, please ensure the time cost of the selected algorithm would not exceed it!
The estimated time costs of the following algorithms are below. Consider the time cost wisely when selecting the algorithm, it is critical but less important than the performance when the time cost does not exceed the waiting time. As long as the timecost difference is not that large (> 1 min), you should pivot more on the performance.

CDNOD: 1.8 minutes
IAMBnPC: 9.6 minutes
PC: 1.0 minutes
FCI: 1.0 minutes
GRaSP: 5.1 minutes

===============================================
Detailed Profiles of the algorithm candidates are shown here. You MUST actively combine and reason with them:

======================================

# CDNOD

Below is a structured profile of the CDNOD (Constraint-based causal Discovery from Nonstationary or heterogeneous Data) algorithm, following the seven dimensions specified in the meta-prompt. The information presented integrates the provided hyperparameter settings, benchmarking results, and external knowledge. 

────────────────────────────────────────────────────────────
1. Hyper-Parameters Sensitivity
────────────────────────────────────────────────────────────

• Number of Key Hyperparameters  
  - CDNOD’s primary hyperparameters include:  
    1. α (alpha): The significance level for conditional independence tests.  
    2. indep_test: The choice of independence test (e.g., “fisherz,” “chisq,” “gsq,” “kci,” etc.).  
    3. depth: The maximum depth in the skeleton discovery phase.  
  - These three hyperparameters—alpha, indep_test, and depth—are generally the most influential in determining both the algorithm’s accuracy and runtime.

• Tuning Difficulty  
  - Default settings are typically sufficient for many domains (e.g., alpha=0.05, indep_test="fisherz," depth=-1). However, the selected values can significantly affect performance when sample sizes or graph sizes are extreme.  
  - The guidelines for alpha (e.g., 0.1 for <500 samples, 0.05 for mid-range, 0.01 for very large samples) offer straightforward, data-driven rules. This means domain experts or automated routines can tune alpha more reliably given the sample size.  
  - For the independence test, recommended defaults exist depending on data type. Selecting the appropriate test (e.g., “kci” for nonlinear continuous) can be done without deep statistical expertise, but advanced users can refine the choice if data assumptions are violated.

• Sensitivity  
  - Alpha exerts a substantial influence on the graph’s sparsity: smaller alpha yields sparser graphs, reducing false positives but potentially missing weaker edges. Larger alpha leads to denser graphs, possibly increasing false positives.  
  - Depth affects the thoroughness of the skeleton search. A high (or unlimited) depth can improve accuracy on complex graphs but may sharply increase runtime. Lower depth settings speed up computations while potentially missing longer-range conditional dependencies.

• Critique/Extension  
  - Hyperparameters controlling the search complexity (e.g., depth) most strongly influence runtime and can also affect correctness if restricted too aggressively.  
  - Hyperparameters tied to statistical tests (alpha, indep_test) have a more direct effect on false positives/negatives. In practice, balancing both sets of parameters is essential for robust performance.

────────────────────────────────────────────────────────────
2. Robustness & Scalability
────────────────────────────────────────────────────────────

• Tolerance to Bad Data Quality  
  - Missing Data: CDNOD is not natively specialized in sophisticated imputation, but it can accommodate partial missingness if the user or a preprocessing routine properly handles or imputes missing entries. The benchmarking results suggest it performs very well overall in missing-data scenarios (ranking near the top in that category).  
  - Measurement/Observation Error: Benchmarks also indicate strong tolerance to moderate or even severe noise, placing CDNOD among the more robust methods tested in handling measurement error.

• Tolerance to Sparse/Dense Connected Systems  
  - While not extensively detailed in the provided reports, constraint-based methods typically show stable performance on sparse networks. Dense networks can still be handled, but the computational load and potential for spurious edges may grow. Depth constraints can partially mitigate these issues.

• Scalability  
  - Among the tested scenarios, CDNOD’s scalability ranking was moderate (somewhere around the middle), hinting that performance remains feasible for medium-to-large graphs, although it might not be among the fastest for extremely large variable counts.  
  - Practical thresholds depend on the machine’s computational resources and the hyperparameters used. Limiting depth and using faster independence tests (e.g., “fastkci” or “rcit”) can help scale to bigger problems.

• Critique/Extension  
  - Parallelization: Constraint-based methods (including CDNOD) can often be parallelized by splitting conditional independence tests across computing cores. This can improve runtime for large datasets, although built-in parallelization support may vary by implementation.  
  - Approximation options like restricting the skeleton search depth or using approximate tests (e.g., “rcit”) strike a balance between runtime and accuracy.

────────────────────────────────────────────────────────────
3. Mixed Data & Complex Functions
────────────────────────────────────────────────────────────

• Noise Type  
  - CDNOD does not assume strictly Gaussian noise. It supports a variety of independence tests (including kernel-based nonparametric methods), making it suitable for non-Gaussian scenarios.

• Mixed Data (Continuous & Discrete)  
  - The recommended “gsq” or “chisq” tests indicate that the algorithm can handle discrete or mixed data. Defaults suggest “fisherz” for purely continuous data, “chisq” for discrete, “gsq” for simplified mixed data, and advanced options (like “kci”) for nonlinear or more complex distributions.

• Heterogeneous Data  
  - CDNOD is specifically designed to address nonstationary and heterogeneous conditions, which is one of its defining strengths (confirmed by top performance in the “Heterogeneity” category of the provided benchmarks).

• Complex Functional Forms  
  - When the relationships between variables are nonlinear, switching to tests such as “kci,” “fastkci,” or “rcit” helps to capture complex dependencies. CDNOD’s framework supports these tests with minimal configuration changes.

• Critique/Extension  
  - If users initially rely on purely linear tests (e.g., “fisherz”) for data that are strongly nonlinear, they risk underspecifying relationships. Therefore, domain knowledge or preliminary data checks can guide the choice of a more robust independence test.  
  - Overfitting can occur if alpha is set too high while a flexible, nonlinear test is used. Proper emphasis on cross-validation or domain-driven alpha choices is advisable.

────────────────────────────────────────────────────────────
4. Computational Complexity
────────────────────────────────────────────────────────────

• Theoretical Time Complexity  
  - As a constraint-based algorithm, skeleton discovery and orientation typically have a complexity that grows with both the number of variables and the maximum conditioning set size. A simplified notation for the worst-case complexity can be denoted as:  
    <temp>[O(p^k)]</temp>  
  where p is the number of variables and k depends on the search depth. Exact exponents vary based on independence test complexity and data sample size.

• Variability in Practical Usage  
  - If depth is set to -1 (unlimited), the search can become computationally heavy for large graphs. Reducing it to smaller values (e.g., 1–3) often substantially cuts runtime.  
  - Selecting faster independence tests (e.g., “fastkci” vs. “kci”) also yields tangible efficiency gains with minor trade-offs in accuracy.

• Critique/Extension  
  - In real-world datasets with many variables, worst-case complexity can be mitigatingly high, but typical performance can be significantly better when the underlying network is not extremely dense.  
  - Hardware-wise, parallel processing can provide meaningful improvements by distributing independence tests across CPU cores.

────────────────────────────────────────────────────────────
5. Interpretability
────────────────────────────────────────────────────────────

• Output Format  
  - CDNOD typically produces a directed acyclic graph (DAG) or partially directed graph (CPDAG) representing causal structures. It may also provide adjacency matrices and edge confidence scores, depending on the implementation.

• Strength of the Output Format  
  - Graphical outputs are straightforward for users to interpret, with edges representing putative causal directions. Some implementations offer p-values or confidence measures that accompany edge findings.

• Limitations of the Output Format  
  - Like most constraint-based methods, certain edges can remain unoriented if the data are insufficient or if the relevant conditional independence tests are ambiguous.  
  - The underlying changing distribution (nonstationarity) can sometimes complicate orientation, leading to partial orientation in complex scenarios.

• Critique/Extension  
  - Domain experts often refine or prune orientations after the algorithm’s initial output. In contexts like multi-stage or time-varying processes, additional domain knowledge can greatly enhance interpretability.

────────────────────────────────────────────────────────────
6. Assumptions
────────────────────────────────────────────────────────────

• Critical Assumptions  
  - Markov and Faithfulness: Variables follow standard causal discovery assumptions that d-separations correspond to conditional independencies.  
  - Causal Sufficiency (with a twist): CDNOD assumes no unobserved confounders that cannot be partly captured by domain indicators or time indices in nonstationary settings.  
  - Nonstationarity/Heterogeneity: CDNOD leverages changes in distribution to help identify causal directions.

• Violation Impact  
  - If truly hidden confounders exist that are unrelated to domain/time indicators, performance may degrade or lead to incorrect orientations.  
  - If the data deviate heavily from the faithfulness assumption, false positives or false negatives can increase in the learned graph.

• Critique/Extension  
  - Extensions of CDNOD can handle partially missing or ambiguous domain indices.  
  - In practice, moderate violations of the assumptions (e.g., mild confounding) might still yield useful causal insights, but caution is warranted.

────────────────────────────────────────────────────────────
7. (Optional) Real-World Benchmarks
────────────────────────────────────────────────────────────

• Performance on Real Datasets  
  - According to some demonstrations (e.g., financial market data applications), CDNOD performs successfully in capturing dynamic causal structures underlying changing market conditions.  
  - Benchmarks indicate it is among the stronger algorithms in contexts involving noise, measurement error, and missing data. In those areas, CDNOD scored near the top in overall performance.

• Practical Tips  
  - For large sample sizes (>10,000), use a lower alpha (e.g., α=0.01) and consider a smaller depth to control runtime.  
  - If data are suspected of containing strongly nonlinear relationships, opt for “kci” or a faster variant (“fastkci” or “rcit”) to improve detection of complex causal links.  
  - Domain-driven insights can help refine or interpret partially oriented edges, especially in real-world settings like finance, biology, or social sciences.

────────────────────────────────────────────────────────────
Summary
────────────────────────────────────────────────────────────

CDNOD is specifically designed for causal discovery in nonstationary and heterogeneous data. Its ability to handle various data types (continuous, discrete, mixed), along with flexible independence test choices, makes it adaptable across multiple domains. While its scalability is not the absolute highest, careful selection of key hyperparameters—particularly depth and the independence test—can keep runtime manageable. As with most constraint-based methods, interpretability is straightforward in graph form, though some edges may remain unoriented without further domain knowledge. Its robust performance in the face of measurement error, missing data, and heterogeneous sources underscores its value for complex real-world applications where distributions shift across time or other conditions.

────────────────────────────────────────────────────────
Benchmarking Results
────────────────────────────────────────────────────────

• Comparative Performance
  – The benchmarking compared 19 different causal discovery algorithms across multiple scenarios.
  – Each algorithm was evaluated on performance (accuracy), efficiency (runtime), and composite metrics, which are represented as level scores from 1-5, with 5 being the best.
  – Levels scores of composite metrics combine performance and efficiency (weights: 0.8 and 0.2 respectively).
[NOTE] The ranking is smaller, the better. The level score is higher, the better.

• Algorithm Rankings

| Scenario | Rank (Mean) | Rank (Std Dev) | Performance (Level) | Efficiency (Level) | Composite (Level) |
|----------|-----------|---------|-------------|------------|------------|
| Scalability | 9.4 | 2.29 | 2.0 | 5.0 | 3.0 |
| Heterogeneity | 9.5 | 1.50 | 2.0 | 4.0 | 2.0 |
| Measurement Error | 11.0 | 0.00 | 3.0 | 4.0 | 2.0 |
| Noise Type | 13.0 | 2.00 | 1.0 | 4.0 | 2.0 |
| Missing Data | 9.2 | 1.79 | 2.0 | 5.0 | 2.0 |
| Edge Probability | 10.0 | 0.82 | 3.0 | 4.0 | 3.0 |
| Discrete Ratio | 10.7 | 1.25 | 2.0 | 4.0 | 2.0 |

• Analysis

  – Overall mean ranking across 7 scenarios: 10.40
  – Average standard deviation: 1.38



======================================

# IAMBnPC

Below is a comprehensive seven-dimensional profile of the IAMBnPC algorithm, integrating the provided hyperparameter specifications, benchmarking statistics, and external/archival information about how this algorithm operates in practice.

━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
1. Hyper-Parameters Sensitivity
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
• Number of Key Hyperparameters  
  - Based on the provided hyperparameter dictionary (File #1) and external sources (File #3), IAMBnPC has two primary hyperparameters that most strongly affect results:  
    1) α (alpha), the significance level for independence testing.  
    2) The independence test selection (indep_test), which can vary (fisherz, chisq, etc.) depending on data type.  
  - Some implementations also expose a “max.sx” parameter (often described in community documentation and academic references) that controls the maximum conditioning set size, which can be crucial in higher-dimensional setups.

• Tuning Difficulty  
  - The significance level (alpha) has suggested defaults (e.g., 0.05 for moderate samples, 0.1 for smaller, 0.01 for very large samples) (File #1). These guidelines simplify tuning for standard use cases, but domain sense or iterative experimentation is beneficial for optimal results.  
  - The independence test parameter (indep_test) has clear default recommendations (fisherz for continuous data, chisq for discrete, etc.). In practice, a domain expert or an automated tool can select tests effectively once the data types (continuous, discrete, or mixed) and anticipated nonlinearities are identified.  
  - max.sx requires more advanced tuning as it directly influences computational effort and can demand deeper domain knowledge to avoid overly large conditioning sets.

• Sensitivity  
  - Small changes in alpha can noticeably alter the sparsity of the discovered structure: lowering alpha (e.g., from 0.05 to 0.01) typically yields fewer edges, while raising it runs the risk of extra false positives.  
  - Switching from a linear independence test (e.g., fisherz) to a nonlinear test (e.g., kci) can significantly extend runtime but often improves detection of complex relationships. Benchmarks (File #2) show that more robust or nonlinear tests can bring moderate decreases in efficiency but may help maintain performance under complex data conditions.

• Critique/Extension  
  - Parameters like alpha relate primarily to statistical tests, determining how conservative or permissive the algorithm is in drawing edges.  
  - By contrast, parameters such as max.sx or the choice of test method can have a major bearing on computational complexity in the graph-search phase. Tuning each in tandem is key to balancing runtime and accuracy.

━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
2. Robustness & Scalability
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
• Tolerance to Bad Data Quality  
  - Missing Data: IAMBnPC does not have a specialized built-in mechanism for handling missingness; most implementations rely on either casewise deletion or test-specific adjustments. According to benchmark observations (File #2), the algorithm’s performance in scenarios with moderate missing data typically remains acceptable, but more severe missingness can degrade inference reliability.  
  - Measurement/Observation Error: Benchmark statistics (File #2) suggest the algorithm ranks around the middle-to-lower range when measurement error is severe, indicating sensitivity to inaccuracies in the independence tests.

• Tolerance to Sparse/Dense Connected Systems  
  - Sparse Graphs: IAMBnPC often excels in sparse settings because fewer edges reduce the conditioning set searches, making it easier to identify Markov Blankets accurately.  
  - Dense Graphs: As density grows, the algorithm’s computational load can increase. It generally still performs competitively if the sample size is sufficient, but it may be slower in identifying all relevant edges accurately.

• Scalability  
  - Benchmarks (File #2) show that under moderate problem sizes, IAMBnPC scales adequately, though it does not always place at the top in efficiency.  
  - Very large numbers of variables or extremely large sample sizes can stress runtime, especially if max.sx or more complex tests (e.g., kci) are used. Memory usage can also become a bottleneck, but partial parallelization of independence tests may mitigate some performance issues.

• Critique/Extension  
  - Parallelization strategies for the independence tests can help handle large data sets faster, an approach mentioned in community discussions (File #3).  
  - Some extensions implement approximate tests or heuristic-based constraint pruning to better cope with large or noisy data environments.

━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
3. Mixed Data & Complex Functions
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
• Noise Type  
  - IAMBnPC does not strictly require Gaussian noise; it adopts whichever independence test is chosen (File #1). This flexibility allows for non-Gaussian or even nonparametric tests (e.g., kci), albeit at added computational cost.

• Mixed Data (Continuous & Discrete)  
  - As per File #1, the suggested test for discrete variables is chisq, and for mixed variables, gsq or other methods (gsq, kci in certain hybrid contexts). This indicates built-in support for analyzing mixed data types, as long as the user selects an appropriate test.

• Heterogeneous Data  
  - Benchmarks (File #2) show moderate reliability when data are heterogeneous (the algorithm’s ranking is neither the highest nor the lowest in that regard). Much depends on how well the chosen test handles distribution shifts and varied variable types.

• Complex Functional Forms  
  - If a nonlinear independence test (like kci) is selected (File #1), IAMBnPC can detect non-linear relationships. Default linear tests (e.g., fisherz) work well but may miss intricate dependencies.

• Critique/Extension  
  - By default, many IAMBnPC implementations use parametric tests (fisherz or chisq) that assume linear or categorical relationships. Users dealing with strongly nonlinear phenomena might consider kci or rcit for better detection, though these methods require more computational time and possibly larger sample sizes to remain stable.

━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
4. Computational Complexity
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
• Theoretical Time Complexity  
  - <temp>[O(|MB(T)| × N)]</temp>, where |MB(T)| is the size of the Markov Blanket of the target T, and N is the overall number of variables. In practice, this can vary if the algorithm iterates repeatedly to refine the Markov Blanket.

• Variability in Practical Usage  
  - Increased max.sx or using more complex independence tests can increase runtime considerably.  
  - Benchmark data (File #2) suggest that IAMBnPC tends to occupy a moderate position in efficiency: not the fastest for extremely large networks, but still viable for typical mid-to-large-scale scenarios.

• Critique/Extension  
  - Real-world usage indicates that worst-case performance rises if the underlying Markov Blanket includes many variables, making the search space large.  
  - Some open-source implementations can leverage multi-core systems to test multiple candidates in parallel, improving real-world runtime.

━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
5. Interpretability
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
• Output Format  
  - IAMBnPC typically identifies a Markov Blanket for a target variable, which includes parents, children, and parents of children (spouses). In more extended usage, it can be used sequentially for each variable, approximating a larger causal structure in the form of adjacency lists or adjacency matrices.

• Strength of the Output Format  
  - Markov blankets are highly interpretable, especially in domain-focused tasks (e.g., finding key predictors). Some implementations provide conditional independence p-values, adding numeric confidence to the adjacency information.

• Limitations of the Output Format  
  - By design, Markov Blanket discovery alone does not fully orient all edges (e.g., distinguishing the parent from the child can require an additional causal orientation step or a separate backward phase).  
  - If sample size is small or alpha is too lenient, false positives may appear in the Markov Blanket, reducing clarity.

• Critique/Extension  
  - Domain experts often post-process the discovered Markov Blankets (e.g., verifying directions or removing improbable edges).  
  - Community resources (File #3) recommend cross-referencing these results with domain constraints for added orientation confidence.

━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
6. Assumptions
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
• Critical Assumptions  
  - Markov Condition: The causal structure in question encodes all conditional independencies present in the data.  
  - Faithfulness: All and only the independencies found in the data are represented in the graph (File #3).  
  - Causal Sufficiency: No unmeasured confounding variables relevant to the included variables.

• Violation Impact  
  - Failure of faithfulness (e.g., strong cancellations or nonlinear confounding) can lead to spurious or missing edges.  
  - Hidden confounders (violating causal sufficiency) may result in flawed Markov Blanket identification.

• Critique/Extension  
  - Some advanced variations relax faithfulness assumptions, allowing for approximate independence detection.  
  - In presence of suspected hidden variables, a user might need to adopt extended methods (e.g., latent variable detection) or adopt additional domain knowledge to correct for confounders.

━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
7. (Optional) Real-World Benchmarks
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
• Performance on Real Datasets  
  - While synthetic experiments provide clarity on alpha’s influence and Markov Blanket sizes, real-world comparisons usually find IAMBnPC near or somewhat above middle-tier performance in terms of both accuracy and runtime (File #2).  
  - In bioinformatics (e.g., gene regulatory networks), IAMBnPC is often praised for effectively identifying candidate regulators for a given gene.

• Practical Tips  
  - Employ domain knowledge whenever possible to set alpha and refine conditioning sets. This often curbs false positives in the Markov Blanket.  
  - If data are high-dimensional or heavily nonlinear, consider advanced (nonlinear) tests and parallelization where available.  
  - Users should be mindful of the assumptions (faithfulness, causal sufficiency), as real-world violations can degrade reliability. Complementing IAMBnPC with domain-specific heuristics or knowledge can help avert misinterpretation.

━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
Summary
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
IAMBnPC is a Markov Blanket discovery algorithm that integrates the iterative conditional independence testing strategy of IAMB with backward-phase refinements inspired by the PC algorithm. Its primary hyperparameter (alpha) controls the strictness of conditional independence tests; tuning this in conjunction with the independence test choice (e.g., fisherz, chisq, kci) is crucial. Though generally robust and interpretable, its performance can diminish with high data noise, missingness, or unfaithful structures. Parallelization or approximate testing can improve scalability, and domain knowledge often helps prune extraneous edges and increase interpretability. Overall, IAMBnPC is a solid and often competitive choice for applications—especially those where identifying relevant predictors for a target variable is paramount—provided its underlying assumptions align reasonably well with the data-generating process.

────────────────────────────────────────────────────────
Benchmarking Results
────────────────────────────────────────────────────────

• Comparative Performance
  – The benchmarking compared 19 different causal discovery algorithms across multiple scenarios.
  – Each algorithm was evaluated on performance (accuracy), efficiency (runtime), and composite metrics, which are represented as level scores from 1-5, with 5 being the best.
  – Levels scores of composite metrics combine performance and efficiency (weights: 0.8 and 0.2 respectively).
[NOTE] The ranking is smaller, the better. The level score is higher, the better.

• Algorithm Rankings

| Scenario | Rank (Mean) | Rank (Std Dev) | Performance (Level) | Efficiency (Level) | Composite (Level) |
|----------|-----------|---------|-------------|------------|------------|
| Scalability | 7.4 | 3.90 | 4.0 | 3.0 | 4.0 |
| Heterogeneity | 3.5 | 2.69 | 4.0 | 5.0 | 5.0 |
| Measurement Error | 6.8 | 0.83 | 3.0 | 5.0 | 4.0 |
| Noise Type | 8.5 | 0.50 | 3.0 | 5.0 | 3.0 |
| Missing Data | 3.8 | 2.49 | 5.0 | 3.0 | 5.0 |
| Edge Probability | 6.0 | 1.41 | 4.0 | 5.0 | 4.0 |
| Discrete Ratio | 6.7 | 0.94 | 4.0 | 5.0 | 4.0 |

• Analysis

  – Overall mean ranking across 7 scenarios: 6.08
  – Average standard deviation: 1.82



======================================

# PC

Below is a seven-dimension profile of the PC (Peter–Clark) algorithm, integrating (1) the hyperparameter file, (2) the benchmarking results, and (3) external knowledge about the algorithm and its typical use cases. References or paraphrased remarks from the external sources are indicated where relevant.

────────────────────────────────────────────────────────────────────────
1. HYPER-PARAMETERS SENSITIVITY
────────────────────────────────────────────────────────────────────────
• Number of Key Hyperparameters  
  - The PC algorithm has three main hyperparameters:  
    1. α (alpha): Significance level for conditional independence tests.  
    2. indep_test: Choice of conditional independence test (e.g., Fisher’s Z, chi-square, G-square, KCI variants).  
    3. depth: Maximum depth for the skeleton search phase.  

• Tuning Difficulty  
  - α is typically the most critical parameter, with recommended defaults in the provided file (e.g., 0.05 for moderate sample sizes, 0.1 for smaller samples, 0.01 for very large samples). These guidelines simplify tuning for different dataset sizes.  
  - The choice of indep_test can be straightforward (Fisher’s Z for continuous data, chi-square for discrete data, etc.), though advanced options (like KCI) may require more specialized knowledge.  
  - depth has a default of -1 (unlimited), but it can be restricted to reduce runtime for large graphs. The suggested rule of thumb scales with the number of nodes, setting smaller depths for larger graphs.  

• Sensitivity  
  - Small changes in α can produce noticeable effects on edge detection: lower α yields more conservative edge selection (fewer false positives) but may lead to more false negatives.  
  - Shifting from an unlimited depth (depth = -1) to a restricted depth (like 1 to 3) can significantly speed up the search on large graphs but may miss subtler causal relationships.  
  - Changing the independence test (e.g., from a linear Fisher test to a non-parametric test like KCI) can likewise alter both runtime and the ability to capture non-linear causal links.  

• Critique/Extension  
  - Parameters that control graph-search complexity (depth) can drastically reduce runtime but may compromise completeness in highly connected graphs.  
  - The α threshold has more direct influence on statistical testing; even small shifts in α can change the number of edges found. Hence, domain knowledge is often helpful to choose a good α.  

────────────────────────────────────────────────────────────────────────
2. ROBUSTNESS & SCALABILITY
────────────────────────────────────────────────────────────────────────
• Tolerance to Bad Data Quality  
  - Benchmarks (File #2) indicate that PC’s performance tends to degrade when data quality drops (e.g., missing data or measurement error). In fact, PC ranked relatively low in measurement error and missing data tolerance, suggesting it does not handle either type of bad data as robustly as some other methods.  
  - It lacks built-in methods for imputation; in practice, users might preprocess missing values or use more advanced PC variants (e.g., PC-missing).  

• Tolerance to Sparse vs. Dense Connected Systems  
  - PC often performs better on sparse systems, since the number of conditional independence tests remains more manageable. For dense graphs, the algorithm may require many tests, increasing the risk of both false positives and elevated runtime.  
  - From a benchmarking standpoint, when confronted with moderately dense networks, performance declines more in runtime than in accuracy, highlighting the combinatorial explosion of tests in denser graphs.  

• Scalability  
  - According to File #2, the algorithm scored moderately on scalability, reflecting that it can handle dozens to a few hundred variables well, but may slow down noticeably beyond that range.  
  - Parallelization or restricting depth can mitigate the combinatorial explosion in larger problems but may reduce the completeness of causal edges.  

• Critique/Extension  
  - Parallel-PC implementations exist and can be employed on multi-core hardware to improve speed.  
  - Using approximation strategies (like a lower maximum depth or partitioning variables) can be beneficial when the dataset contains hundreds or thousands of variables, though these techniques might weaken correctness guarantees.  

────────────────────────────────────────────────────────────────────────
3. MIXED DATA & COMPLEX FUNCTIONS
────────────────────────────────────────────────────────────────────────
• Noise Type  
  - The PC algorithm’s traditional setup often presupposes approximately Gaussian errors for continuous data, but other independence tests allow for broader noise distributions.  
  - Benchmarks (File #2) suggested that PC performed somewhat lower when confronted with more diverse or non-Gaussian noise, although advanced tests (KCI, RCIT) can address non-linearity.  

• Mixed Data (Continuous & Discrete)  
  - PC can integrate different conditional independence tests: Fisher’s Z or partial correlation for continuous variables; chi-square or G-square for discrete; or a mix for hybrid data.  
  - Some PC implementations provide direct support for mixed data via specialized tests like G-square, allowing simultaneous handling of continuous and categorical variables.  

• Heterogeneous Data  
  - Benchmark results (File #2) show it scored in the middle ranges under “Heterogeneity.” It can handle moderate changes in distribution across variables, provided a suitable test is chosen.  
  - Marked shifts across subsets of data may require domain-driven adjustments or advanced versions (like FCI for partial latent confounding).  

• Complex Functional Forms  
  - The standard PC algorithm is tailored for linear (or near-linear) dependencies, but employing a kernel-based test (e.g., KCI) can detect non-linear relationships.  
  - Non-linear extensions usually come with higher computational burdens, which can be partially offset by “fastKCI” or “RCIT.”  

• Critique/Extension  
  - Users often underestimate how crucial the independence test choice is when dealing with non-linear patterns. A robust non-parametric test can significantly improve detection of non-linear causal links.  
  - Overfitting concerns can arise in small samples if a highly flexible test is used without proper regularization or multiple testing corrections.  

────────────────────────────────────────────────────────────────────────
4. COMPUTATIONAL COMPLEXITY
────────────────────────────────────────────────────────────────────────
• Theoretical Time Complexity  
  - The PC algorithm can have a worst-case time complexity of <temp>[O(n^(k+2))]</temp>, where n is the number of variables and k is the maximum degree of a node in the true graph.  

• Variability in Practical Usage  
  - Despite the polynomial (sometimes high-order) worst-case bound, many real-world graphs are sparse, so runtime is often much lower.  
  - Increasing depth or lowering α can drive up the number of tests, and thus runtime can spike. Conversely, restricting depth dampens the combinatorial explosion but risks missing some edges.  

• Critique/Extension  
  - The worst-case scenario might be rarely encountered in practical sparse settings. However, in denser structures or with large depth parameters, runtime can indeed grow significantly.  
  - Parallelization can help distribute the skeleton-discovery phase across multiple CPUs, improving the typical runtime on large datasets.  

────────────────────────────────────────────────────────────────────────
5. INTERPRETABILITY
────────────────────────────────────────────────────────────────────────
• Output Format  
  - PC outputs a CPDAG (Completed Partially Directed Acyclic Graph). This graph shows which edges are definitely directed, as well as edges uncertain about orientation.  
  - Some implementations also provide adjacency matrices with confidence measures (e.g., p-values).  

• Strength of the Output Format  
  - CPDAGs are considered quite interpretable: they clarify which directions are inferred and which remain undetermined.  
  - Many software packages (e.g., Tetrad, pcalg) provide visualization tools to help interpret a CPDAG.  

• Limitations of the Output Format  
  - Edges in the “partially” directed region can generate ambiguity, as the algorithm leaves some edges unoriented if the data are insufficiently informative.  
  - In high-dimensional or complex domains, the resulting graph might still be large and cumbersome to interpret without additional domain knowledge.  

• Critique/Extension  
  - Domain experts frequently refine or manually orient ambiguous edges, especially in specialized fields such as genomics or neuroscience.  
  - Some suggest performing a stability or bootstrap analysis to highlight edges that are consistently inferred across subsamples.  

────────────────────────────────────────────────────────────────────────
6. ASSUMPTIONS
────────────────────────────────────────────────────────────────────────
• Critical Assumptions  
  - Causal Markov condition: Each variable is conditionally independent of its non-descendants, given its parents.  
  - Faithfulness: All and only the conditional independencies in the true structure are reflected in the data.  
  - Causal sufficiency: No unobserved confounders of the measured variables.  
  - Acyclicity: The underlying causal structure forms a Directed Acyclic Graph (DAG).  

• Violation Impact  
  - Missing confounders can lead to spurious or missing edges.  
  - Unfaithful data-generating mechanisms can hide true edges or create extra independencies.  
  - Feedback loops or cyclic structures break the acyclicity assumption, causing the algorithm to infer incomplete or incorrect edges.  

• Critique/Extension  
  - When hidden variable confounding is suspected, variants like FCI (Fast Causal Inference) or RFCI are recommended instead of basic PC.  
  - Many real-world systems include some feedback or non-stationary behavior, so partial violations are not uncommon. Users often need to test sensitivity or adopt extended algorithms.  

────────────────────────────────────────────────────────────────────────
7. (OPTIONAL) REAL-WORLD BENCHMARKS
────────────────────────────────────────────────────────────────────────
• Performance on Real Datasets  
  - Across a variety of real datasets (e.g., gene-expression data, brain connectivity data), PC tends to perform competitively for structure learning, provided the data are not extremely noisy or riddled with missing values.  
  - In some evaluations, PC was among the more accurate approaches for sparse graphs but was sometimes outperformed by score-based or hybrid methods on very noisy or dense data.  

• Practical Tips  
  - Combining PC with domain knowledge (e.g., known biological pathways) often yields improved orientation of edges.  
  - Limiting depth can drastically reduce computation time for high-dimensional data, but practitioners must confirm that the trade-off in missed edges is acceptable.  
  - Discussion forums and user groups consistently recommend verifying the sample size is sufficient for the chosen α level, or considering small-sample corrections if data are limited.  

────────────────────────────────────────────────────────────────────────
CONCLUSION
────────────────────────────────────────────────────────────────────────
The PC algorithm remains a flagship approach in constraint-based causal discovery, prized for its interpretability (CPDAG output) and relatively direct hyperparameter tuning guidelines (notably α and depth). It is best applied in scenarios where:  
• The graph is not excessively dense.  
• The data generally meet Markov and faithfulness assumptions.  
• There are no severe missing data or measurement error problems, or these issues are pre-processed/handled externally.  
Recent developments in parallelization, alternative independence tests, and specialized PC variants help address non-linearities, mixed data, and large-scale settings. Nonetheless, potential users must keep in mind the algorithm’s sensitivity to significance thresholds, data assumptions, and the combinatorial explosion of conditional tests in bigger or denser networks.

────────────────────────────────────────────────────────
Benchmarking Results
────────────────────────────────────────────────────────

• Comparative Performance
  – The benchmarking compared 19 different causal discovery algorithms across multiple scenarios.
  – Each algorithm was evaluated on performance (accuracy), efficiency (runtime), and composite metrics, which are represented as level scores from 1-5, with 5 being the best.
  – Levels scores of composite metrics combine performance and efficiency (weights: 0.8 and 0.2 respectively).
[NOTE] The ranking is smaller, the better. The level score is higher, the better.

• Algorithm Rankings

| Scenario | Rank (Mean) | Rank (Std Dev) | Performance (Level) | Efficiency (Level) | Composite (Level) |
|----------|-----------|---------|-------------|------------|------------|
| Scalability | 8.1 | 2.47 | 3.0 | 5.0 | 3.0 |
| Heterogeneity | 8.2 | 1.30 | 3.0 | 5.0 | 3.0 |
| Measurement Error | 9.0 | 0.00 | 3.0 | 5.0 | 3.0 |
| Noise Type | 10.5 | 1.50 | 2.0 | 5.0 | 2.0 |
| Missing Data | 6.5 | 1.80 | 3.0 | 4.0 | 4.0 |
| Edge Probability | 7.7 | 3.40 | 3.0 | 5.0 | 3.0 |
| Discrete Ratio | 10.0 | 0.82 | 3.0 | 5.0 | 3.0 |

• Analysis

  – Overall mean ranking across 7 scenarios: 8.58
  – Average standard deviation: 1.61



======================================

# FCI

Below is a comprehensive profile of the Fast Causal Inference (FCI) algorithm, structured according to the seven “degrees” (dimensions) of analysis. This profile synthesizes:  
• File #1 (the hyperparameter settings),  
• File #2 (benchmarking results),  
• External knowledge about FCI (including peer-reviewed papers, publicly available documentation, and community discussions),  
• General principles of causal discovery.  

────────────────────────────────────────────────────────────────────
1. Hyper-Parameters Sensitivity
────────────────────────────────────────────────────────────────────

• Number of Key Hyperparameters  
  – FCI has three principal hyperparameters (based on File #1):  
    1) α (significance level),  
    2) The choice of independence test (indep_test),  
    3) The depth parameter controlling how exhaustively conditional independence tests are performed.  

• Tuning Difficulty  
  – α (significance level) is among the most impactful parameters. Its default is often 0.05, though File #1 suggests using 0.1 for very small samples (< 500), 0.05 for moderate-sized samples (500 to 10,000), and 0.01 for extremely large samples (> 10,000).  
  – The independence test parameter (indep_test) depends on data type and modeling assumptions. Commonly used options in practice include:  
     “fisherz” for continuous data under a Gaussian assumption,  
     “chisq” for discrete data,  
     “gsq,” “kci,” or “rcit” for more general or non-linear data.  
  – Depth settings can be unrestricted (-1) or limited to reduce computation. While the defaults are straightforward (e.g., unlimited depth for small graphs), tuning ultimately depends on computational constraints and how dense the graph might be.  

• Sensitivity  
  – α: Small changes (e.g., from 0.05 to 0.01) can sharply reduce false positives but potentially increase false negatives. Benchmarks in File #2 indicate that performance metrics (e.g., adjacency precision) shift when α is made more conservative.  
  – Depth: Restricting depth can speed up the skeleton discovery step considerably but risks missing some indirect connections, as fewer conditional sets are tested.  

• Critique/Extension  
  – Parameters that control search complexity (e.g., depth) heavily influence runtime, particularly on large or dense graphs.  
  – Parameters controlling statistical tests (e.g., α and the independence test selection) most directly affect the quality of the discovered causal structure (false positives / false negatives in edges).

────────────────────────────────────────────────────────────────────
2. Robustness & Scalability
────────────────────────────────────────────────────────────────────

• Tolerance to Bad Data Quality  
  – Missing Data: FCI can tolerate missing data if the chosen independence test supports it (e.g., some tests impute or ignore missing cases). Benchmarks in File #2 show FCI performing around a moderate level (neither the best nor the worst) when data are missing.  
  – Measurement/Observation Error: FCI’s performance decreases with severe noise but remains relatively stable for moderate noise levels. Its ability to discover latent structures can mitigate some confounding effects of measurement error, though the precision of edges can drop if noise is extreme.  

• Tolerance to Sparse/Dense Connected Systems  
  – Sparse Networks: FCI typically performs well in sparse graphs, because fewer edges mean fewer complex conditional independence tests.  
  – Dense Networks: FCI can handle denser connectivity but at notably increased computational cost, as more conditional tests are required and orienting edges becomes more complex.  

• Scalability  
  – As the number of variables grows, the number of required conditional independence tests can explode. File #2 reports an above-average computational burden for FCI once the variable count is large.  
  – FCI can still be applied to moderately high-dimensional data, but runtime may become prohibitive without restricting the depth parameter or using parallelization strategies.  

• Critique/Extension  
  – Parallel Implementation: Some implementations offer parallelized independence testing to help with large datasets.  
  – Approximate or Bounded Depth: Restricting the maximum depth of search is a common strategy to reduce runtime (though this may sacrifice some accuracy).

────────────────────────────────────────────────────────────────────
3. Mixed Data & Complex Functions
────────────────────────────────────────────────────────────────────

• Noise Type  
  – FCI itself does not strictly require Gaussian noise; it simply needs an appropriate conditional independence test (e.g., “kci” or “rcit” from File #1) to detect non-linear, non-Gaussian dependencies.  

• Mixed Data (Continuous & Discrete)  
  – Users can choose independence tests suited to mixed data (e.g., “gsq”) or generalized tests that handle a variety of variable types. According to File #1, “kci,” “fastkci,” and “rcit” may also address nonlinear interactions across data types.  

• Heterogeneous Data  
  – File #2 suggests moderate performance under heterogeneous conditions (e.g., data collected from different sources or distributions). FCI’s separation-of-independences approach can still work if independence tests remain valid across distributions.  

• Complex Functional Forms  
  – Non-linear relationships are detectable if the chosen independence test can pick them up. FCI, however, does not automatically model these relationships; it relies on test outcomes.  

• Critique/Extension  
  – The primary limitation is reliance on an accurate independence test. If the test poorly captures non-linear or complex interactions, FCI may miss true edges or introduce extraneous ones.  
  – Some advanced versions of FCI or user-developed variations incorporate kernel-based tests, enabling more robust detection of complicated functional relationships.

────────────────────────────────────────────────────────────────────
4. Computational Complexity
────────────────────────────────────────────────────────────────────

• Theoretical Time Complexity  
  – FCI is often cited as having an exponential worst-case time complexity with respect to the number of variables. A simplified expression is presented here: <temp>[O(2^n)]</temp>.  
  – In practice, the real cost is heavily influenced by how many adjacency constraints it identifies early in the process.

• Variability in Practical Usage  
  – Depth-limited FCI can reduce runtime to something more manageable (potentially polynomial in many real-world scenarios).  
  – Higher significance levels (larger α) may reduce the total number of conditional independence tests because edges are removed more slowly, but they risk more spurious edges that must be tested and oriented later.  

• Critique/Extension  
  – The worst-case complexity is rarely reached in sparse, real-world networks.  
  – Parallel computing and caching of test results (mentioned in community forums) can improve runtime substantially.

────────────────────────────────────────────────────────────────────
5. Interpretability
────────────────────────────────────────────────────────────────────

• Output Format  
  – FCI outputs a Partial Ancestral Graph (PAG), which represents causal constraints among variables, including potential latent confounders.  

• Strength of the Output Format  
  – PAGs explicitly encode uncertain edges (e.g., bidirected edges indicating hidden confounders, circle endpoints indicating ambiguous directions). This offers rich information about causal possibilities.  

• Limitations of the Output Format  
  – PAGs can be less intuitive than DAGs or CPDAGs. Edges may remain unoriented or partially oriented, especially if data alone cannot resolve their direction.  
  – Users often require additional domain knowledge to interpret or further refine certain ambiguous edges.  

• Critique/Extension  
  – Researchers recommend domain experts consult the PAG’s edge marks to hypothesize plausible latent variables or refine uncertain orientations.  
  – Visualization libraries for FCI exist but require a solid grasp of the edge nomenclature.

────────────────────────────────────────────────────────────────────
6. Assumptions
────────────────────────────────────────────────────────────────────

• Critical Assumptions  
  – Markov and Faithfulness: The distribution of the data must align with the “true” causal graph’s conditional independencies, without pathological cancellations.  
  – No Cycles: FCI assumes acyclicity among observed variables, even though it allows for the possibility of latent confounders.  
  – Causal Sufficiency is relaxed compared to simpler algorithms (like PC), meaning FCI can account for possible hidden variables.  

• Violation Impact  
  – Violating faithfulness can produce incomplete or misleading PAGs (e.g., missing edges or unvalued circle endpoints).  
  – When hidden confounding is extremely strong and the sample size is low, the discovered PAG can have many ambiguities.  

• Critique/Extension  
  – FCI is often chosen because it explicitly relaxes causal sufficiency assumptions. Indeed, that is a main strength over simpler methods.  
  – Partial violations of faithfulness can degrade accuracy but do not necessarily invalidate the overall skeleton of the PAG.

────────────────────────────────────────────────────────────────────
7. Real-World Benchmarks
────────────────────────────────────────────────────────────────────

• Performance on Real Datasets  
  – File #2 indicates that FCI often performs at a middle or upper-middle rank across different metrics (e.g., adjacency precision, sensitivity to hidden variables).  
  – Empirical studies (from external references) report that FCI often outperforms algorithms that assume no latent confounders when hidden variables or selection bias are indeed present.

• Practical Tips  
  – Adjust α based on sample size: Larger samples can justify smaller α for fewer false positives; smaller samples may require a higher α to avoid missing genuine edges.  
  – Parallelizing the conditional independence tests or limiting depth can make FCI more tractable for large-dimensional datasets.  
  – Domain knowledge plays a crucial role in resolving uncertain orientations in the PAG, ensuring the final model is more interpretable and actionable.  

• Common Pitfalls  
  – Using an independence test misaligned with the data’s underlying distribution (e.g., only using Fisher’s Z in the presence of non-linear or discrete variables) can lead to incorrect edges.  
  – Overly restrictive significance levels with small sample sizes can cause many missed edges, while overly lenient significance levels can flood the output with spurious edges.

────────────────────────────────────────────────────────────────────
Summary
────────────────────────────────────────────────────────────────────
FCI (Fast Causal Inference) is a widely respected causal discovery algorithm that relaxes the assumption of no hidden confounders, making it especially valuable when latent variables or selection bias may be present. Its key hyperparameters—significance level (α), independence test choice, and depth constraint—all substantially affect how many edges are retained and how computationally heavy the procedure becomes. Although FCI can handle both continuous and discrete data (and even mixed types) via different independence tests, its runtime grows quickly with the number of variables and the complexity of the network.

Because FCI outputs a Partial Ancestral Graph (PAG), it can reveal potential latent confounding, but this representation may contain uncertain or bidirected edges requiring careful interpretation. This complexity can also make it challenging to tune parameters optimally without domain or statistical expertise. When properly configured, FCI has proven effective in real-world applications like genetics, climate science, and social networks—domains where hidden confounders often lurk. Overall, its flexibility and ability to account for unobserved variables are balanced by higher computational demands and some interpretability challenges relative to simpler methods.

────────────────────────────────────────────────────────
Benchmarking Results
────────────────────────────────────────────────────────

• Comparative Performance
  – The benchmarking compared 19 different causal discovery algorithms across multiple scenarios.
  – Each algorithm was evaluated on performance (accuracy), efficiency (runtime), and composite metrics, which are represented as level scores from 1-5, with 5 being the best.
  – Levels scores of composite metrics combine performance and efficiency (weights: 0.8 and 0.2 respectively).
[NOTE] The ranking is smaller, the better. The level score is higher, the better.

• Algorithm Rankings

| Scenario | Rank (Mean) | Rank (Std Dev) | Performance (Level) | Efficiency (Level) | Composite (Level) |
|----------|-----------|---------|-------------|------------|------------|
| Scalability | 6.1 | 1.96 | 4.0 | 4.0 | 4.0 |
| Heterogeneity | 5.0 | 0.71 | 4.0 | 4.0 | 5.0 |
| Measurement Error | 6.5 | 0.50 | 4.0 | 4.0 | 4.0 |
| Noise Type | 8.0 | 2.00 | 3.0 | 4.0 | 3.0 |
| Missing Data | 7.5 | 4.39 | 4.0 | 2.0 | 3.0 |
| Edge Probability | 6.3 | 2.05 | 4.0 | 4.0 | 4.0 |
| Discrete Ratio | 7.0 | 0.82 | 4.0 | 4.0 | 4.0 |

• Analysis

  – Overall mean ranking across 7 scenarios: 6.64
  – Average standard deviation: 1.78



======================================

# GRaSP

Below is a consolidated profile of the GRaSP causal discovery algorithm, organized by the seven degrees (dimensions) specified. The profile draws on:  
• File #1: The algorithm’s hyperparameters (score_func, depth)  
• File #2: Benchmarking results showing how GRaSP performed under different conditions (e.g., missing data, measurement error, noise)  
• File #3: External information, which (in this case) includes references to a “Graph-based Residue neighborhood Strategy to Predict binding sites”—likely a domain-specific usage of a related or similarly named method. We incorporate relevant insights on graph-based methods and performance, while focusing on causal discovery aspects.

────────────────────────────────────────────────────────────────────────
1. HYPER-PARAMETERS SENSITIVITY
────────────────────────────────────────────────────────────────────────
• Number of Key Hyperparameters  
  GRaSP, as described in File #1, has two critical hyperparameters: (1) score_func and (2) depth.  
  – score_func determines the local scoring metric (e.g., BIC, BDeu, local_score_marginal_general, local_score_marginal_multi).  
  – depth controls how thoroughly the algorithm searches permutations in the graph space.

• Tuning Difficulty  
  – score_func: There are suggested defaults (e.g., local_score_BIC) that provide reasonable performance for many linear or moderately complex data types. For truly discrete data, local_score_BDeu may be preferred; for nonparametric or nonlinear settings, local_score_marginal_general or local_score_marginal_multi can be used but are more computationally demanding.  
  – depth: File #1 suggests different depth values depending on the number of variables (e.g., depth=5 for small graphs, depth=3 for medium, depth=2 for large). Although these guidelines are straightforward, picking an optimal depth can still require domain knowledge. In principle, an experienced analyst (or an LLM, if guided by domain constraints) can tune these effectively.

• Sensitivity  
  – score_func: Shifting from a simple score (like BIC) to a highly flexible, nonparametric score (like marginal_general) can drastically increase runtime but may give more accurate structure in complex data.  
  – depth: Small increases in depth can cause exponentially increased search, leading to better-fitting structures but raising computational load. Benchmarks from File #2 (Scalability results) suggest GRaSP remains efficient at moderate depths, but with very large depth settings, runtime can balloon.

• Critique/Extension  
  – Graph-search parameters (depth) directly affect computational complexity and can have a large impact on runtime.  
  – Statistical scoring parameters (e.g., BIC vs. marginal scores) mainly affect estimation quality and can help capture non-linearities but demand more computational resources.

────────────────────────────────────────────────────────────────────────
2. ROBUSTNESS & SCALABILITY
────────────────────────────────────────────────────────────────────────
• Tolerance to Bad Data Quality  
  – Missing Data: According to File #2 (Missing Data results), GRaSP scored highly on performance and reasonably high on efficiency (both in the upper tier among tested methods). This implies that GRaSP’s approach, possibly through local scoring or partial data strategies, is relatively robust when data are incomplete.  
  – Measurement/Observation Error: From File #2 (Measurement Error), GRaSP also performed near the top tier in accommodating moderate to somewhat severe noise, suggesting that its scoring strategy retains stability under data perturbations.

• Tolerance to Sparse/Dense Connected Systems  
  – Although the raw numeric rankings in File #2 are not repeated here, the results indicate that GRaSP can adapt well to both highly connected (dense) and modestly connected networks. There is some performance improvement in more moderately dense systems, but it does not degrade dramatically in sparser networks.  
  – Part of this tolerance may stem from the local scoring approach, which can isolate independent relationships without requiring the entire graph to be extremely dense or strictly sparse.

• Scalability  
  – File #2 (Scalability) shows GRaSP scoring relatively well on performance and somewhat lower on raw efficiency costs compared to certain lightweight methods. In practical terms, it can handle a moderate number of variables effectively, but extremely large graphs (e.g., hundreds of nodes) may require careful setting of depth and possible approximations.  
  – External references (File #3) about “residue neighborhood strategies” suggest that the graph-based approach has shown scalability in specific applications (e.g., residue-level analysis). While domain-specific, it corroborates that a well-chosen graph representation can handle substantial data if tuned properly.

• Critique/Extension  
  – If the dataset is huge (many dozens or hundreds of variables), one might enable parallelization (if available) or reduce depth to keep runtime manageable.  
  – Approximate search heuristics can potentially mitigate the exponential explosion from increasing depth.

────────────────────────────────────────────────────────────────────────
3. MIXED DATA & COMPLEX FUNCTIONS
────────────────────────────────────────────────────────────────────────
• Noise Type  
  – The default local_score_BIC often assumes data with near-Gaussian residuals. However, File #1 explicitly introduces nonparametric scoring options (local_score_marginal_general, local_score_marginal_multi) that can handle non-Gaussian distributions.  
  – File #2 (Noise Type) indicates that GRaSP is among the stronger methods tested, retaining good structure accuracy when the noise distribution deviates from simple Gaussian assumptions.

• Mixed Data (Continuous & Discrete)  
  – From File #1, the presence of local_score_BDeu suggests built-in handling of discrete data. local_score_marginal_* also suggests more flexible scoring for potentially mixed data.  
  – File #3’s domain-specific references to combining discrete amino acid features with continuous spatial data imply that GRaSP’s approach can extend to both data types, although that usage is domain-specific.

• Heterogeneous Data  
  – In File #2 (Heterogeneity), GRaSP again shows strong composite performance, suggesting it adapts well to data from multiple sources or distributions, at least compared to many other methods tested.  
  – True distribution shifts (e.g., entirely different data-generating processes over time) may require additional caution. However, the benchmark outcomes hint that GRaSP remains stable if the data remain partially consistent in underlying structure.

• Complex Functional Forms  
  – The local_score_marginal_general option indicates that GRaSP can capture non-linear relationships by leveraging more flexible, cross-validated scoring.  
  – This comes at a computational cost, so users typically reserve these advanced scorings for strongly non-linear data.

• Critique/Extension  
  – By default, “local_score_BIC” or “BDeu” is more linear/parametric. Users seeking to model more intricate causal dependencies should consider the nonparametric scores.  
  – Overfitting concerns can arise if the nonparametric scoring and deep search depth are used simultaneously on small datasets. Practical guidelines usually involve domain knowledge to keep complexity in check.

────────────────────────────────────────────────────────────────────────
4. COMPUTATIONAL COMPLEXITY
────────────────────────────────────────────────────────────────────────
• Theoretical Time Complexity  
  – While no exact formula is given in the files, the iterative or combinatorial structure of a permutation-based search typically grows super-polynomially with graph size (especially as depth increases). We can approximate it as:  
    <temp>[theoretical_time_complexity]</temp>  
    for example, near O(n^d) or worse, where n is the number of variables and d is the depth parameter.

• Variability in Practical Usage  
  – File #2 implies that practical runtimes can vary significantly based on how large depth gets. With depth = 5 (intended for smaller graphs), the algorithm can thoroughly explore structures but may become slow for bigger networks.  
  – Using local_score_marginal_general or local_score_marginal_multi also increases runtime compared to simpler scores like BIC or BDeu.

• Critique/Extension  
  – In typical scenarios (moderate n, moderate depth), GRaSP is close to the upper tier in structural accuracy while maintaining feasible runtimes.  
  – Worst-case behavior can be quite large (combinatorial search), so applying parallelization or heuristics is recommended for very large variable counts.

────────────────────────────────────────────────────────────────────────
5. INTERPRETABILITY
────────────────────────────────────────────────────────────────────────
• Output Format  
  – GRaSP outputs a directed graph (often a DAG or partially directed structure), which can also be summarized in adjacency matrices with edges reflecting presumed causal directions. If some edges remain ambiguous, it may provide partially directed edges.

• Strength of the Output Format  
  – Directed or partially directed graphs are standard in causal discovery, which is generally comprehensible to analysts.  
  – Power users can supplement these graphs with confidence scores or local p-values if the scoring function supports it.

• Limitations of the Output Format  
  – If hidden confounders exist and are not modeled, some edges may remain unoriented or spurious.  
  – File #3’s references to “residue neighborhood” usage in a different domain do not address interpretability in the typical sense of cause-effect graphs. Nonetheless, the underlying graph structure can be understandable to domain experts with the right visualization tools.

• Critique/Extension  
  – Domain expert validation is often recommended to confirm or refine edge orientations.  
  – Community forums (as occasionally referenced in File #3) sometimes suggest post-processing steps to remove uncertain edges or incorporate prior knowledge.

────────────────────────────────────────────────────────────────────────
6. ASSUMPTIONS
────────────────────────────────────────────────────────────────────────
• Critical Assumptions  
  – Markov assumption: Each variable is independent of its non-descendants given its parents in the DAG.  
  – Faithfulness: The observational distributions reflect the underlying graphical d-separations.  
  – Causal sufficiency: Typically assumes all relevant causal variables are included (no unobserved confounders).  
  – Homogeneity: The relationship (structural, functional form) doesn’t change drastically across the dataset.

• Violation Impact  
  – If hidden confounders exist, edges might be misattributed.  
  – File #2’s strong performance under moderate noise suggests some robustness, but severe violations (e.g., completely different data-generating processes in subpopulations) can degrade results.

• Critique/Extension  
  – Some advanced versions of causal algorithms allow detection of hidden variables or partial relaxation of faithfulness. GRaSP’s main settings (per the hyperparameters in File #1) focus on observed variables only.  
  – Mild assumption violations typically cause moderate inaccuracies, but more severe ones (e.g., major unobserved confounders) can cause structural errors.

────────────────────────────────────────────────────────────────────────
7. REAL-WORLD BENCHMARKS
────────────────────────────────────────────────────────────────────────
• Performance on Real Datasets  
  – In File #2, GRaSP did consistently in the higher range of tested methods across several dimensions, such as handling missing data and moderate measurement error.  
  – External references in File #3 (though more about ligand-binding site prediction) do show success in complex, domain-intensive tasks. This suggests that if GRaSP is similarly structured for causal discovery, it can scale to real-world, intricate data scenarios and still produce meaningful graphs.

• Practical Tips  
  – For large data, reduce depth or use parallel/approximate searching to maintain feasible runtimes.  
  – Nonparametric scores (marginal_general/marginal_multi) are powerful but should be used judiciously to avoid overfitting in smaller sample sizes.  
  – Domain knowledge (e.g., known causal pathways) can help refine or confirm discovered edges.

• Pitfalls / Known Limitations  
  – Over-reliance on deep searches or nonparametric scores can skyrocket computation time.  
  – If major confounders remain unobserved, the discovered structure might be misleading.

────────────────────────────────────────────────────────────────────────
CONCLUSION
────────────────────────────────────────────────────────────────────────
GRaSP emerges as a flexible causal discovery algorithm with two central hyperparameters—score_func (ranging from standard BIC/BDeu to more advanced nonparametric scores) and depth (controlling search breadth). Benchmarks indicate robust performance under missing data, noise, and heterogeneous conditions, although runtime can escalate when depth is set too high or when nonparametric scoring is enabled for large datasets. The algorithm’s output is a directed or partially directed graphical structure, which is generally interpretable but may require domain expertise to confirm edge orientations. Like most causal discovery tools, GRaSP assumes no unobserved confounders and that the data-generating processes follow certain Markov and faithfulness properties. Recent external references to a similarly named graph-based method in protein-ligand binding prediction suggest that graph-centric approaches can excel in complex, real-world settings—underscoring GRaSP’s promise for broader domain applications, provided that users are mindful of hyperparameter tuning and assumption checks.

────────────────────────────────────────────────────────
Benchmarking Results
────────────────────────────────────────────────────────

• Comparative Performance
  – The benchmarking compared 19 different causal discovery algorithms across multiple scenarios.
  – Each algorithm was evaluated on performance (accuracy), efficiency (runtime), and composite metrics, which are represented as level scores from 1-5, with 5 being the best.
  – Levels scores of composite metrics combine performance and efficiency (weights: 0.8 and 0.2 respectively).
[NOTE] The ranking is smaller, the better. The level score is higher, the better.

• Algorithm Rankings

| Scenario | Rank (Mean) | Rank (Std Dev) | Performance (Level) | Efficiency (Level) | Composite (Level) |
|----------|-----------|---------|-------------|------------|------------|
| Scalability | 4.1 | 4.75 | 5.0 | 2.0 | 5.0 |
| Heterogeneity | 6.8 | 3.96 | 4.0 | 2.0 | 4.0 |
| Measurement Error | 1.5 | 0.50 | 5.0 | 2.0 | 5.0 |
| Noise Type | 2.0 | 1.00 | 5.0 | 2.0 | 5.0 |
| Missing Data | 5.5 | 2.69 | 4.0 | 3.0 | 5.0 |
| Edge Probability | 1.3 | 0.47 | 5.0 | 2.0 | 5.0 |
| Discrete Ratio | 1.3 | 0.47 | 5.0 | 2.0 | 5.0 |

• Analysis

  – Overall mean ranking across 7 scenarios: 3.22
  – Average standard deviation: 1.98





===============================================

Think in-depth and thoroughly step by step. Please include the reasoning process, the ultimate reason why the picked algorithm beats the others and finally the selected algorithm in the JSON format. Cite/Quote quantity/number and references for the evidences of analyzing using one specific algorithm or not. Do not return any other text or comments:

{
  "reasoning": "reasoning process",
  "reason": "ultimate reason why it beats the others"
  "algorithm": "algorithm_name",
}
