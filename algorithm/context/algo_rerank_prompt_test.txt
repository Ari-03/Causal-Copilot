You will conduct causal discovery on the Tabular Dataset ./demo_data/20250402_230105/earthquakes/earthquakes.csv containing the following Columns:

year	month	day	richter	area	region	deaths

The Detailed Background Information is listed below:

This is fake domain knowledge for debugging purposes.

The Statistics Information about the dataset is:

The dataset has the following characteristics:

Data Type: The overall data type is Continuous.

The sample size is 123 with 7 features. 

This dataset is not time-series data. 

Data Quality: There are missing values in the dataset.

Statistical Properties:
- Linearity: The relationships between variables are predominantly linear.
- Gaussian Errors: The errors in the data do not follow a Gaussian distribution.
- Heterogeneity: The dataset is not heterogeneous. 





Based on the above information, please select the best-suited algorithm from the following candidate (the order of the algorithm candidates is not important):

dict_keys(['PC', 'DirectLiNGAM', 'InterIAMB'])

===============================================
Note that the user can only wait for 1440.0 minutes for the algorithm execution, please ensure the time cost of the selected algorithm would not exceed it!
The estimated time costs of the following algorithms using linear estimation methods (e.g. Fisherz for constraint-based methods, BIC for score-based methods) are below. Use it as a reference for the algorithm runtime comparison, note that the absolute value might changes significantly when the estimation methods are not linear (e.g. KCI for constraint-based methods, Generalized score for score-based methods). Consider the time cost wisely when selecting the algorithm, it is critical but less important than the performance when the time cost does not exceed the waiting time. As long as the timecost difference is not that large 1440.0, you should pivot more on the performance.

PC: 1.0 minutes
DirectLiNGAM: 1.0 minutes
InterIAMB: 1.0 minutes

At the same time, be careful about the current device availability for GPU/CUDA acceleration. Note that some algorithm settings would need GPU to run, which could be a indirect limitation on the algorithm application potential:


Current machine doesn't support CUDA, do not choose any GPU-powered algorithms.

===============================================
Detailed Profiles of the algorithm candidates are shown here. We also include the supported hyperparamerters as additional information to help you know the potentials of each algorithm combining various hyperparamerter settings. You MUST actively combine and reason with them:

======================================

# PC

Below is a seven-dimension profile of the PC (Peter–Clark) algorithm, integrating (1) the hyperparameter file, (2) the benchmarking results, and (3) external knowledge about the algorithm and its typical use cases. References or paraphrased remarks from the external sources are indicated where relevant.

────────────────────────────────────────────────────────────────────────
1. HYPER-PARAMETERS SENSITIVITY
────────────────────────────────────────────────────────────────────────
• Number of Key Hyperparameters  
  - The PC algorithm has three main hyperparameters:  
    1. α (alpha): Significance level for conditional independence tests.  
    2. indep_test: Choice of conditional independence test (e.g., Fisher's Z, chi-square, G-square, KCI variants) with both CPU and GPU implementations.  
    3. depth: Maximum depth for the skeleton search phase.  

• Tuning Difficulty  
  - α is typically the most critical parameter, with recommended defaults in the provided file (e.g., 0.05 for moderate sample sizes, 0.1 for smaller samples, 0.01 for very large samples). These guidelines simplify tuning for different dataset sizes.  
  - The choice of indep_test can be straightforward (Fisher's Z for continuous data, chi-square for discrete data, etc.), with GPU-accelerated options now available for significantly improved performance. The selection includes 'fisherz_cpu'/'fisherz_gpu' for continuous data, 'chisq_cpu'/'chisq_gpu' for discrete data, and 'cmiknn_gpu' for nonparametric testing with GPU acceleration.
  - depth has a default of -1 (unlimited), but it can be restricted to reduce runtime for large graphs. The suggested rule of thumb scales with the number of nodes, setting smaller depths for larger graphs.  

• Sensitivity  
  - Small changes in α can produce noticeable effects on edge detection: lower α yields more conservative edge selection (fewer false positives) but may lead to more false negatives.  
  - Shifting from an unlimited depth (depth = -1) to a restricted depth (like 1 to 3) can significantly speed up the search on large graphs but may miss subtler causal relationships.  
  - Changing the independence test (e.g., from a linear Fisher test to a non-parametric test like KCI or cmiknn_gpu) can likewise alter both runtime and the ability to capture non-linear causal links.  

• Critique/Extension  
  - Parameters that control graph-search complexity (depth) can drastically reduce runtime but may compromise completeness in highly connected graphs.  
  - The α threshold has more direct influence on statistical testing; even small shifts in α can change the number of edges found. Hence, domain knowledge is often helpful to choose a good α.  
  - The availability of GPU-accelerated tests provides a significant performance boost without sacrificing accuracy, making PC more viable for larger datasets.

────────────────────────────────────────────────────────────────────────
2. ROBUSTNESS & SCALABILITY
────────────────────────────────────────────────────────────────────────
• Tolerance to Bad Data Quality  
  - Benchmarks (File #2) indicate that PC's performance tends to degrade when data quality drops (e.g., missing data or measurement error). In fact, PC ranked relatively low in measurement error and missing data tolerance, suggesting it does not handle either type of bad data as robustly as some other methods.  
  - It lacks built-in methods for imputation; in practice, users might preprocess missing values or use more advanced PC variants (e.g., PC-missing).  

• Tolerance to Sparse vs. Dense Connected Systems  
  - PC often performs better on sparse systems, since the number of conditional independence tests remains more manageable. For dense graphs, the algorithm may require many tests, increasing the risk of both false positives and elevated runtime.  
  - From a benchmarking standpoint, when confronted with moderately dense networks, performance declines more in runtime than in accuracy, highlighting the combinatorial explosion of tests in denser graphs.  

• Scalability  
  - With the integration of GPU-accelerated independence tests ('fisherz_gpu', 'chisq_gpu', and 'cmiknn_gpu'), PC's scalability has significantly improved. The GPU tests provide extreme speedups compared to their CPU counterparts, especially for large datasets.
  - According to the hyperparameter file, 'cmiknn_gpu' offers a 1000x speedup compared to CPU-based 'kci' with comparable accuracy, making nonparametric testing viable for much larger datasets.
  - Even with GPU acceleration, restricting depth remains an important strategy for very large graphs to manage computational complexity.

• Critique/Extension  
  - Parallel-PC implementations exist and can be employed on multi-core hardware to improve speed.  
  - Using approximation strategies (like a lower maximum depth or partitioning variables) can be beneficial when the dataset contains hundreds or thousands of variables, though these techniques might weaken correctness guarantees.  
  - The GPU-accelerated tests make PC much more practical for large-scale applications, addressing one of its traditional limitations.

────────────────────────────────────────────────────────────────────────
3. MIXED DATA & COMPLEX FUNCTIONS
────────────────────────────────────────────────────────────────────────
• Noise Type  
  - The PC algorithm's traditional setup often presupposes approximately Gaussian errors for continuous data, but other independence tests allow for broader noise distributions.  
  - Benchmarks (File #2) suggested that PC performed somewhat lower when confronted with more diverse or non-Gaussian noise, although advanced tests (KCI, RCIT, and now cmiknn_gpu) can address non-linearity.  

• Mixed Data (Continuous & Discrete)  
  - PC can integrate different conditional independence tests: Fisher's Z or partial correlation for continuous variables; chi-square or G-square for discrete; or a mix for hybrid data.  
  - Some PC implementations provide direct support for mixed data via specialized tests like G-square, allowing simultaneous handling of continuous and categorical variables.  
  - Both CPU and GPU implementations are available for different data types, with 'fisherz_cpu'/'fisherz_gpu' for continuous data and 'chisq_cpu'/'chisq_gpu' for discrete data.

• Heterogeneous Data  
  - Benchmark results (File #2) show it scored in the middle ranges under "Heterogeneity." It can handle moderate changes in distribution across variables, provided a suitable test is chosen.  
  - Marked shifts across subsets of data may require domain-driven adjustments or advanced versions (like FCI for partial latent confounding).  

• Complex Functional Forms  
  - The standard PC algorithm is tailored for linear (or near-linear) dependencies, but employing kernel-based tests can detect non-linear relationships.
  - The GPU-accelerated 'cmiknn_gpu' test provides a powerful nonparametric option for detecting complex, non-linear relationships with significantly improved computational efficiency compared to CPU-based alternatives like 'kci_cpu'.
  - Other non-linear options include 'fastkci_cpu' (faster than kci) and 'rcit_cpu' (fastest approximation of kci), each with their own trade-offs between speed and accuracy.

• Critique/Extension  
  - Users often underestimate how crucial the independence test choice is when dealing with non-linear patterns. A robust non-parametric test can significantly improve detection of non-linear causal links.  
  - Overfitting concerns can arise in small samples if a highly flexible test is used without proper regularization or multiple testing corrections.  
  - The addition of GPU-accelerated tests, especially 'cmiknn_gpu', makes non-linear causal discovery much more practical for larger datasets.

────────────────────────────────────────────────────────────────────────
4. COMPUTATIONAL COMPLEXITY
────────────────────────────────────────────────────────────────────────
• Theoretical Time Complexity  
  - The PC algorithm can have a worst-case time complexity of <temp>[O(n^(k+2))]</temp>, where n is the number of variables and k is the maximum degree of a node in the true graph.  

• Variability in Practical Usage  
  - Despite the polynomial (sometimes high-order) worst-case bound, many real-world graphs are sparse, so runtime is often much lower.  
  - Increasing depth or lowering α can drive up the number of tests, and thus runtime can spike. Conversely, restricting depth dampens the combinatorial explosion but risks missing some edges.  
  - The GPU-accelerated tests ('fisherz_gpu', 'chisq_gpu', 'cmiknn_gpu') dramatically reduce the practical runtime, especially for large datasets, making PC much more viable for real-world applications.

• Critique/Extension  
  - The worst-case scenario might be rarely encountered in practical sparse settings. However, in denser structures or with large depth parameters, runtime can indeed grow significantly.  
  - GPU acceleration provides a substantial practical improvement in computational efficiency without changing the theoretical complexity, making PC more competitive with other algorithms for large-scale causal discovery.

────────────────────────────────────────────────────────────────────────
5. INTERPRETABILITY
────────────────────────────────────────────────────────────────────────
• Output Format  
  - PC outputs a CPDAG (Completed Partially Directed Acyclic Graph). This graph shows which edges are definitely directed, as well as edges uncertain about orientation.  
  - Some implementations also provide adjacency matrices with confidence measures (e.g., p-values).  

• Strength of the Output Format  
  - CPDAGs are considered quite interpretable: they clarify which directions are inferred and which remain undetermined.  
  - Many software packages (e.g., Tetrad, pcalg) provide visualization tools to help interpret a CPDAG.  

• Limitations of the Output Format  
  - Edges in the "partially" directed region can generate ambiguity, as the algorithm leaves some edges unoriented if the data are insufficiently informative.  
  - In high-dimensional or complex domains, the resulting graph might still be large and cumbersome to interpret without additional domain knowledge.  

• Critique/Extension  
  - Domain experts frequently refine or manually orient ambiguous edges, especially in specialized fields such as genomics or neuroscience.  
  - Some suggest performing a stability or bootstrap analysis to highlight edges that are consistently inferred across subsamples.  

────────────────────────────────────────────────────────────────────────
6. ASSUMPTIONS
────────────────────────────────────────────────────────────────────────
• Critical Assumptions  
  - Causal Markov condition: Each variable is conditionally independent of its non-descendants, given its parents.  
  - Faithfulness: All and only the conditional independencies in the true structure are reflected in the data.  
  - Causal sufficiency: No unobserved confounders of the measured variables.  
  - Acyclicity: The underlying causal structure forms a Directed Acyclic Graph (DAG).  

• Violation Impact  
  - Missing confounders can lead to spurious or missing edges.  
  - Unfaithful data-generating mechanisms can hide true edges or create extra independencies.  
  - Feedback loops or cyclic structures break the acyclicity assumption, causing the algorithm to infer incomplete or incorrect edges.  

• Critique/Extension  
  - When hidden variable confounding is suspected, variants like FCI (Fast Causal Inference) or RFCI are recommended instead of basic PC.  
  - Many real-world systems include some feedback or non-stationary behavior, so partial violations are not uncommon. Users often need to test sensitivity or adopt extended algorithms.  

────────────────────────────────────────────────────────────────────────
7. (OPTIONAL) REAL-WORLD BENCHMARKS
────────────────────────────────────────────────────────────────────────
• Performance on Real Datasets  
  - Across a variety of real datasets (e.g., gene-expression data, brain connectivity data), PC tends to perform competitively for structure learning, provided the data are not extremely noisy or riddled with missing values.  
  - In some evaluations, PC was among the more accurate approaches for sparse graphs but was sometimes outperformed by score-based or hybrid methods on very noisy or dense data.  

• Practical Tips  
  - Combining PC with domain knowledge (e.g., known biological pathways) often yields improved orientation of edges.  
  - Limiting depth can drastically reduce computation time for high-dimensional data, but practitioners must confirm that the trade-off in missed edges is acceptable.  
  - Discussion forums and user groups consistently recommend verifying the sample size is sufficient for the chosen α level, or considering small-sample corrections if data are limited.  
  - For large datasets or when dealing with non-linear relationships, GPU-accelerated tests ('fisherz_gpu', 'chisq_gpu', 'cmiknn_gpu') are strongly recommended for their significant performance benefits.

────────────────────────────────────────────────────────────────────────
CONCLUSION
────────────────────────────────────────────────────────────────────────
The PC algorithm remains a flagship approach in constraint-based causal discovery, prized for its interpretability (CPDAG output) and relatively direct hyperparameter tuning guidelines (notably α and depth). With the integration of GPU-accelerated independence tests, it has become much more scalable and practical for large datasets. It is best applied in scenarios where:  
• The graph is not excessively dense.  
• The data generally meet Markov and faithfulness assumptions.  
• There are no severe missing data or measurement error problems, or these issues are pre-processed/handled externally.  
• GPU acceleration is available for large datasets or when using nonparametric tests for non-linear relationships.

Recent developments in GPU acceleration, alternative independence tests, and specialized PC variants help address non-linearities, mixed data, and large-scale settings. The 'cmiknn_gpu' test in particular provides a 1000x speedup for nonparametric testing compared to CPU-based alternatives, making PC much more competitive for complex, non-linear causal discovery tasks. Nonetheless, potential users must keep in mind the algorithm's sensitivity to significance thresholds, data assumptions, and the combinatorial explosion of conditional tests in bigger or denser networks.

────────────────────────────────────────────────────────
Benchmarking Results
────────────────────────────────────────────────────────

• Comparative Performance
  – The benchmarking compared 19 different causal discovery algorithms across multiple scenarios.
  – Each algorithm was evaluated on performance (accuracy), efficiency (runtime), and composite metrics, which are represented as level scores from 1-5, with 5 being the best.
  – Levels scores of composite metrics combine performance and efficiency (weights: 0.8 and 0.2 respectively).
[NOTE] The ranking is smaller, the better. The level score is higher, the better.

• Algorithm Rankings

| Scenario | Rank (Mean) | Rank (Std Dev) | Performance (Level) | Efficiency (Level) | Composite (Level) |
|----------|-----------|---------|-------------|------------|------------|
| Scalability | 8.1 | 2.47 | 3.0 | 5.0 | 3.0 |
| Heterogeneity | 8.2 | 1.30 | 3.0 | 5.0 | 3.0 |
| Measurement Error | 9.0 | 0.00 | 3.0 | 5.0 | 3.0 |
| Noise Type | 10.5 | 1.50 | 2.0 | 5.0 | 2.0 |
| Missing Data | 6.5 | 1.80 | 3.0 | 4.0 | 4.0 |
| Edge Probability | 7.7 | 3.40 | 3.0 | 5.0 | 3.0 |
| Discrete Ratio | 10.0 | 0.82 | 3.0 | 5.0 | 3.0 |

• Analysis

  – Overall mean ranking across 7 scenarios: 8.58
  – Average standard deviation: 1.61


## Supported hyperparameters: {
    "algorithm_name": "PC",
    "alpha": {
        "meaning": "Desired significance level in (0, 1)",
        "available_values": {
            "default": 0.05,
            "small_sample": 0.1,
            "large_sample": 0.01
        },
        "expert_suggestion": "Adjust based on sample size, more conservative (lower) values for larger samples. If < 500, use 0.1; Else if 500-10000 (<10000 but >500), use 0.05; Else if > 10000, using 0.01."
    },
    "indep_test": {
        "meaning": "Independence test method",
        "available_values": {
            "default": "fisherz_cpu",
            "continuous_cpu": "fisherz_cpu",
            "continuous_gpu": "fisherz_gpu",
            "discrete_cpu": "chisq_cpu",
            "discrete_gpu": "chisq_gpu",
            "robust_nonlinear_cpu": "kci_cpu",
            "robust_nonlinear_gpu": "cmiknn_gpu",
            "fast_robust_nonlinear_cpu": "fastkci_cpu",
            "approximate_fast_nonlinear_cpu": "rcit_cpu"
        },
        "expert_suggestion": "Choose based on data type and hardware. CPU TESTS: 'fisherz_cpu' for linear continuous data; 'chisq_cpu' for discrete data (only applied for pure discrete data); 'kci_cpu' for nonlinear data (very slow, use only with variable size < 15 and sample size < 1500); 'fastkci_cpu' is faster than kci (use with < 20 variables and sample size < 3000); 'rcit_cpu' is the fastest approximation of kci (use with < 30 variables and sample size < 5000). GPU TESTS: 'fisherz_gpu' and 'chisq_gpu' (only applied for pure discrete data) work similarly but are extremely fast because of GPU's super parallel computing; 'cmiknn_gpu' is a GPU-accelerated nonparametric test that provides 1000x speedup compared to CPU-based 'kci' with comparable accuracy. GPU acceleration is strongly recommended for large datasets."
    },
    "depth": {
        "meaning": "Maximum depth for skeleton search",
        "available_values": {
            "default": -1,
            "small_graph": 6,
            "medium_graph": 4,
            "large_graph": 2,
            "extra_large_graph": 1
        },
        "expert_suggestion": "Use -1 for unlimited depth. For large graphs, limiting depth (e.g., 1-3) can significantly speed up the algorithm at the cost of some accuracy. A graph with node number < 10, use depth 6; A graph with node number 10 - 25, use depth 4; A graph with node number 25-50, use depth 2; A graph with node number > 50, use depth 1."
    }
}

======================================

# DirectLiNGAM

Below is a detailed profile of DirectLiNGAM according to the requested seven dimensions. The information comes from:  
• File #1 (Hyperparameter JSON)  
• File #2 (Benchmarking results)  
• File #3 (External knowledge, including research papers, library docs, and community discussions)  
• General knowledge about causal discovery methods  

────────────────────────────────────────────────────────────────
1. Hyper-Parameters Sensitivity
────────────────────────────────────────────────────────────────

• Number of Key Hyperparameters  
  – From File #1: DirectLiNGAM exposes relatively few critical hyperparameters. The main one is the “measure” used for independence checks, which can be set to "pwling" (default), "kernel", or potentially other variants.  

• Tuning Difficulty  
  – The default “pwling” measure generally works well in practice. Switching to "kernel" can help model more complex non-linearities but may require domain knowledge or experimentation to confirm usefulness.  
  – Because the core parameter set is small, most users find it straightforward to adopt default settings. An LLM or a modestly experienced user can typically manage hyperparameter tuning without extensive domain expertise.  

• Sensitivity  
  – Changing the “measure” parameter can have a noticeable effect on both runtime and estimation quality. For example, the kernel-based approach may capture non-linear effects but can be slower for larger datasets.  
  – Relatively small changes to other parameters (e.g., random seeds) do not typically affect causal estimates dramatically; they mainly influence reproducibility rather than the final structure.  

• Critique/Extension  
  – Parameters governing the independence measure directly affect how the algorithm assesses statistical relationships. By contrast, parameters related to search heuristics or other complexities (if present) could alter the graph-search process. Because DirectLiNGAM’s parameter set is focused on the independence measure, the main tuning burden lies there.  

────────────────────────────────────────────────────────────────
2. Robustness & Scalability
────────────────────────────────────────────────────────────────

• Tolerance to Bad Data Quality  
  – Missing Data: File #2 suggests that DirectLiNGAM performed relatively poorly in robustness tests that introduced missing observations, ranking near the lower end among compared methods. It typically requires complete-case analysis or other imputation strategies since it does not natively handle missing data.  
  – Measurement/Observation Error: The same benchmarking (File #2) indicates that measurement error also has a stronger degrading effect on DirectLiNGAM compared to some other methods, though the magnitude depends on the severity of the noise.  

• Tolerance to Sparse/Dense Connected Systems  
  – Empirically, DirectLiNGAM can work with both sparse and moderately dense networks. In external forums (File #3), users note that the algorithm can handle networks of varying degrees of connectivity so long as the linear and non-Gaussian assumptions hold.  

• Scalability  
  – From the scalability benchmarks in File #2, DirectLiNGAM showed moderate performance when the number of variables is not excessively large. However, in higher-dimensional settings, its efficiency can suffer compared to more optimized or approximate search methods.  
  – Some implementations offer faster “pwling” variants (e.g., “pwling_fast”) that can partially mitigate runtime for larger datasets, especially if GPU resources are available.  

• Critique/Extension  
  – Parallelization options exist in certain implementations for the independence measure calculation, which can help in large-scale scenarios but do not change the underlying complexity.  

────────────────────────────────────────────────────────────────
3. Mixed Data & Complex Functions
────────────────────────────────────────────────────────────────

• Noise Type  
  – DirectLiNGAM is predicated on non-Gaussian noise assumptions. This is a defining feature that typically improves its ability to recover causal directions when data deviate from Gaussian distributions.  

• Mixed Data (Continuous & Discrete)  
  – The method is best suited for continuous variables with non-Gaussian errors. It is not natively optimized for discrete or mixed-type variables, so practitioners often resort to discrete-to-continuous transformations or alternative approaches for categorical data.  

• Heterogeneous Data  
  – File #2’s heterogeneity tests showed that DirectLiNGAM can cope with shifts in data distributions, provided the underlying linear non-Gaussian assumptions remain valid. Its relative performance was in a mid-range among all methods tested.  

• Complex Functional Forms  
  – The core algorithm assumes linear relationships. If the real causal processes are substantially non-linear, performance may degrade unless one uses a kernel-based measure or extends the baseline approach with non-linear adaptations.  

• Critique/Extension  
  – Researchers often propose hybrid strategies (e.g., combining DirectLiNGAM with generalized additive models) to better capture non-linearities while retaining the core non-Gaussian principle. Overfitting risks can emerge if kernel-based approaches are used on smaller datasets without careful hyperparameter control.  

────────────────────────────────────────────────────────────────
4. Computational Complexity
────────────────────────────────────────────────────────────────

• Theoretical Time Complexity  
  – <temp>[O(n^4)]</temp> in the worst case, where n is the number of variables, due to the iterative nature of removing exogenous variables and recalculating partial correlations or independence measures.  

• Variability in Practical Usage  
  – Runtime scales unfavorably as n grows large, though in practice, many real-world datasets remain in dimensional ranges where DirectLiNGAM is still tractable.  
  – Certain hyperparameters (e.g., the “kernel” measure) can inflate the computational cost substantially compared to the default “pwling.”  

• Critique/Extension  
  – In typical applications with moderately sized data sets, DirectLiNGAM can be faster than multi-stage iterative search algorithms that re-check all pairwise relationships repeatedly.  
  – Parallelization on modern hardware can reduce the practical runtime but does not eliminate the fundamental worst-case growth.  

────────────────────────────────────────────────────────────────
5. Interpretability
────────────────────────────────────────────────────────────────

• Output Format  
  – DirectLiNGAM outputs a causal ordering among variables and a corresponding adjacency matrix that shows directed edges and their connection strengths. Users can readily translate this matrix into a Directed Acyclic Graph (DAG).  

• Strength of the Output Format  
  – Because the algorithm provides a clear ordering, it generally offers an intuitive explanation of “who causes whom.” This can be especially useful in linear frameworks where the coefficients can be interpreted semiquantitatively.  
  – Some implementations also report confidence estimates or p-values for edges (often via bootstrapping).  

• Limitations of the Output Format  
  – Under certain conditions (e.g., near-Gaussian distributions or violation of causal sufficiency), the orientation of edges might be ambiguous or sensitive to small data perturbations.  
  – The adjacency matrix alone may not capture the uncertainty of orientation if the data only weakly determine the causal direction.  

• Critique/Extension  
  – Domain knowledge is often helpful to confirm or refine the discovered order, especially in real-world applications where hidden confounding might exist.  

────────────────────────────────────────────────────────────────
6. Assumptions
────────────────────────────────────────────────────────────────

• Critical Assumptions  
  – Linear Relations: All direct causal effects are assumed to be linear.  
  – Non-Gaussianity: Error terms are non-Gaussian (with at most one Gaussian error allowed).  
  – Causal Sufficiency: No unobserved confounders that affect multiple variables simultaneously.  
  – Acyclicity: The causal structure is a Directed Acyclic Graph (DAG).  

• Violation Impact  
  – If data contain multiple Gaussian components or strong non-linearities, DirectLiNGAM’s ability to recover correct directions can degrade significantly.  
  – Hidden confounders or feedback loops can lead to erroneous causal claims, as the algorithm relies on the assumption of causal sufficiency and no cycles.  

• Critique/Extension  
  – Some researchers relax assumptions by incorporating latent variable modeling or generalizing the linear model. However, these are not part of the baseline DirectLiNGAM implementation and often require more advanced modifications.  

────────────────────────────────────────────────────────────────
7. Real-World Benchmarks
────────────────────────────────────────────────────────────────

• Performance on Real Datasets  
  – DirectLiNGAM has been applied in fields like economics, genomics, and neuroscience, showing notable success when the non-Gaussian assumption holds. It often outperforms classic methods (e.g., basic ICA-based or constraint-based approaches) in identifying true causal directions in such domains.  
  – Benchmarks reported in File #2 indicate it performs near the middle or slightly lower end when significant measurement error or extensive missing data is present. Conversely, it ranks relatively better when data are of decent quality and conform to linear–non-Gaussian assumptions.  

• Practical Tips  
  – Incorporating domain knowledge (e.g., known cause-effect constraints) alongside DirectLiNGAM outputs can improve results and help confirm uncertain edges.  
  – Users should carefully check the plausibility of linear and non-Gaussian assumptions in their data prior to applying DirectLiNGAM.  
  – Some practitioners recommend using a kernel-based measure only if there is a strong reason to believe in non-linearities; the default “pwling” measure is often sufficient and more computationally efficient.  

────────────────────────────────────────────────────────────────
Summary
────────────────────────────────────────────────────────────────
DirectLiNGAM is a specialized causal discovery algorithm well-suited to scenarios where linearity and non-Gaussianity of errors hold. Its minimal hyperparameter set eases tuning, though the choice of independence measure (especially “pwling” vs. “kernel”) can significantly affect performance and runtime. It is relatively robust to moderate noise but is more sensitive than some alternatives to missing data and measurement error. Its theoretical runtime complexity is on the higher side (O(n^4)), but in practice it remains efficient for moderate-scale problems, especially with fast variants or parallelization.

The algorithm’s outputs—a causal ordering and an adjacency matrix—are generally straightforward to interpret, but they rely on strong assumptions: linear effects, non-Gaussian noise, and no unobserved confounders. In real-world applications where these assumptions hold, DirectLiNGAM can accurately identify causal direction, often surpassing methods that assume Gaussian noise. Users should, however, exercise caution when facing hidden confounders, complex non-linearities, or severe data-quality issues.

────────────────────────────────────────────────────────
Benchmarking Results
────────────────────────────────────────────────────────

• Comparative Performance
  – The benchmarking compared 19 different causal discovery algorithms across multiple scenarios.
  – Each algorithm was evaluated on performance (accuracy), efficiency (runtime), and composite metrics, which are represented as level scores from 1-5, with 5 being the best.
  – Levels scores of composite metrics combine performance and efficiency (weights: 0.8 and 0.2 respectively).
[NOTE] The ranking is smaller, the better. The level score is higher, the better.

• Algorithm Rankings

| Scenario | Rank (Mean) | Rank (Std Dev) | Performance (Level) | Efficiency (Level) | Composite (Level) |
|----------|-----------|---------|-------------|------------|------------|
| Scalability | 11.6 | 4.69 | 1.0 | 5.0 | 2.0 |
| Heterogeneity | 12.8 | 0.83 | 1.0 | 4.0 | 2.0 |
| Measurement Error | 13.8 | 0.43 | 1.0 | 4.0 | 1.0 |
| Noise Type | 7.5 | 6.50 | 3.0 | 4.0 | 4.0 |
| Missing Data | 12.0 | 1.41 | 1.0 | 5.0 | 2.0 |
| Edge Probability | 13.3 | 0.94 | 1.0 | 4.0 | 1.0 |
| Discrete Ratio | 13.7 | 0.47 | 1.0 | 4.0 | 1.0 |

• Analysis

  – Overall mean ranking across 7 scenarios: 12.09
  – Average standard deviation: 2.18



## Supported hyperparameters: {
    "algorithm_name": "DirectLiNGAM",
    "measure": {
        "meaning": "Measure to evaluate independence",
        "available_values": {
            "default": "pwling",
            "kernel": "kernel"
        },
        "expert_suggestion": "Use 'pwling' for pairwise likelihood-based measure, or 'kernel' for kernel-based measure."
    },
    "gpu": {
        "meaning": "Whether to use GPU acceleration.",
        "available_values": {
            "default": false,
            "use_cpu": false,
            "use_gpu": true
        },
        "expert_suggestion": "If GPU is available, set gpu=True to use GPU acceleration. It is recommended to use GPU acceleration for large datasets."
    }
}

======================================

# InterIAMB

Below is an in-depth profile of InterIAMB, organized around the seven dimensions (or “degrees”) specified in the meta-prompt. The following draws on the provided hyperparameter definitions, the comparative benchmarking results, external online information about InterIAMB, and general knowledge of Markov blanket (MB) and causal discovery methods.

────────────────────────────────────────────────────────
1. Hyper-Parameters Sensitivity
────────────────────────────────────────────────────────

• Number of Key Hyperparameters  
  InterIAMB’s foundational hyperparameter is the alpha (α) significance level for conditional independence testing. In practice, there are also choices of which independence test (fisherz, chisq, etc.) to use, but α is by far the most impactful parameter guiding the algorithm. Hence, we can consider two main critical hyperparameters for InterIAMB:  
  1) alpha (the significance threshold).  
  2) indep_test (choice of independence test).  

• Tuning Difficulty  
  – Alpha: Default guidance is straightforward. For small samples (<500), a higher alpha (e.g., 0.1) is often suggested to avoid missing significant edges. For moderate sample sizes (500–10,000), α = 0.05 is a common setting. For very large datasets (>10,000), α = 0.01 is recommended to reduce false positives.  
  – Independence Test: The algorithm offers recommended defaults (e.g., “fisherz” for continuous data, “chisq” for discrete, “gsq” for simpler mixed data). These guidelines limit the tuning difficulty because the user can often select the test based on data type.  

• Sensitivity  
  – Alpha: Small changes to α can shift how many conditional independencies are declared. Lower α typically yields a more conservative MB, with fewer false positives but potentially more false negatives. Higher α can speed up execution slightly (fewer re-checks needed), but can also add spurious edges.  
  – Independence Test Choice: Selecting a more advanced non-linear test (e.g., “kci” or “fastkci”) can improve detection of complex relationships but typically increases computation.  

• Critique/Extension  
  – For InterIAMB, most of the effect is from parameters controlling the significance testing step (especially α), rather than from search complexity parameters (it follows an iterative MB strategy). Thus, adjustments in significance threshold often dominate performance changes in practice.  

────────────────────────────────────────────────────────
2. Robustness & Scalability
────────────────────────────────────────────────────────

• Tolerance to Bad Data Quality  
  – Missing Data: Benchmarks suggest InterIAMB is not particularly robust when data are missing at random in large portions, indicating a drop in performance and efficiency compared to scenarios without missing data. Handling missingness usually requires either imputation or specialized independence tests, neither of which is a built-in feature of the standard InterIAMB formulation.  
  – Measurement/Observation Error: In the presence of moderate noise or errors, InterIAMB tends to remain fairly stable, but severe measurement error can compromise the correctness of the conditional independence checks—leading to more false edges or missing edges in the identified MB.  

• Tolerance to Sparse/Dense Connected Systems  
  – Overall, InterIAMB’s performance is typically quite solid for moderately dense networks. For highly sparse networks, it can sometimes require carefully tuning α since the algorithm might be overly conservative and fail to detect weaker associations. Conversely, in highly dense networks, the iterative MB approach can become more computationally expensive as more variables must be checked for conditional independence.  

• Scalability  
  – Sample Size: InterIAMB can process thousands of samples efficiently, particularly with optimized independence tests. However, extremely large sample sizes (>10,000) often warrant a stricter α to limit false positives.  
  – Number of Variables: InterIAMB improves on the original IAMB, but can still encounter computational bottlenecks with very high-dimensional data. Some parallelized or optimized implementations exist, which help scale to larger variable sets by parallel independence testing.  

• Critique/Extension  
  – Parallelization: Because InterIAMB’s main cost arises from repeated conditional independence tests, parallel or distributed strategies can alleviate runtime issues in large datasets if computing resources permit.  

────────────────────────────────────────────────────────
3. Mixed Data & Complex Functions
────────────────────────────────────────────────────────

• Noise Type  
  – InterIAMB itself does not strictly assume Gaussian noise; rather, the performance depends on the independence tests. If the user chooses “fisherz,” a linear-Gaussian assumption is made. For more general or non-Gaussian data, tests like “kci” or “rcit” allow detecting more complex dependencies.  

• Mixed Data (Continuous & Discrete)  
  – The algorithm can accommodate both types of variables by selecting, for instance, “gsq” or other specialized tests. The recommended practice is to carefully match data types to an appropriate test method so that the underlying assumptions are not violated.  

• Heterogeneous Data  
  – Benchmarks reflect moderate performance for heterogeneous datasets (e.g., multiple types of variables). InterIAMB can handle such data if the independence test is chosen appropriately, but advanced scenarios (massive amounts of unbalanced continuous/discrete variables) might require more carefully tuned hyperparameters or specialized tests.  

• Complex Functional Forms  
  – In principle, InterIAMB can uncover non-linear associations if a corresponding non-linear independence test is used. However, if a purely linear test (“fisherz”) is chosen, strong non-linear relationships may be missed or misinterpreted.  

• Critique/Extension  
  – As a constraint-based method, InterIAMB does not inherently model functional forms; it relies on the independence test’s ability to detect conditional dependencies. To capture very complex relationships, users are advised to select robust non-linear tests (e.g., “kci,” “rcit”).  

────────────────────────────────────────────────────────
4. Computational Complexity
────────────────────────────────────────────────────────

• Theoretical Time Complexity  
  – The complexity is often cited as <temp>[O(n^2)]</temp> in many references, though in practice it can grow faster if repeated independence tests become more extensive for large n or for complex network structures.  

• Variability in Practical Usage  
  – Increasing the number of variables or choosing more computationally heavy tests (e.g., kernel-based ones) can significantly expand runtime. Tighter α thresholds can also add overhead by requiring additional checks to confirm or reject a conditional independence.  
  – In benchmarks, InterIAMB was not the slowest method tested but does experience performance degradation with many variables and repeated conditional testing.  

• Critique/Extension  
  – InterIAMB’s worst-case behavior can be higher than the quoted O(n^2) depending on the network’s connectivity and iterative test expansions. Typical implementations, however, are optimized for average-case performance.  
  – Modern computing platforms (multi-core, GPU) can reduce bottlenecks if code is parallelized for independence tests.  

────────────────────────────────────────────────────────
5. Interpretability
────────────────────────────────────────────────────────

• Output Format  
  – Rather than outputting a full causal structure (like a DAG), InterIAMB focuses on the Markov blanket for each target variable: the minimal set of variables that shield the target from all other variables.  

• Strength of the Output Format  
  – The Markov blanket is highly interpretable: users see exactly which variables are directly relevant (parents, children, and co-parents) to a target. This can be ideal for feature selection or local neighborhood discovery in a causal sense.  

• Limitations of the Output Format  
  – The direction or orientation of edges is not inherently guaranteed. Thus, while InterIAMB indicates local dependencies, it does not by itself fully resolve causal directions or detect hidden confounders.  
  – Confidence metrics or p-values for edges can be parsed from the conditional independence tests, but are not always aggregated in a single “score.”  

• Critique/Extension  
  – For broader causal conclusions, many users combine InterIAMB with a separate orientation step (e.g., a scoring-based method or domain-expert input). This pipeline approach often yields improved interpretability of causal relations.  

────────────────────────────────────────────────────────
6. Assumptions
────────────────────────────────────────────────────────

• Critical Assumptions  
  – Causal Sufficiency: No significant latent confounders that connect variables in unobserved ways.  
  – Markov Condition: Each variable is conditionally independent of its non-descendants given its parents.  
  – Faithfulness (or “No cancellations”): The observed independencies in the data reflect the true underlying causal structure.  

• Violation Impact  
  – Violating causal sufficiency or faithfulness can degrade correctness of the discovered MB. This might result in missing edges or spurious associations if hidden confounders violate these assumptions.  
  – In some community-reported evaluations, small omissions to faithfulness did not drastically degrade InterIAMB, but major violations (e.g., strong confounding) caused significant inaccuracies.  

• Critique/Extension  
  – Certain InterIAMB variants relax these assumptions partially, but the standard InterIAMB remains a constraint-based approach relying heavily on them. Users encountering potential hidden confounding often resort to domain knowledge or extended algorithms for adjustments.  

────────────────────────────────────────────────────────
7. (Optional) Real-World Benchmarks
────────────────────────────────────────────────────────

• Performance on Real Datasets  
  – InterIAMB has performed competitively in several MB discovery benchmarks, frequently matching or outperforming earlier IAMB variants. In moderately sized real-world datasets, it often demonstrates a good balance between precision (avoiding false edges) and recall (identifying true associations).  
  – Compared to specialized high-dimensional methods, InterIAMB can be outpaced when the number of variables becomes extremely large, but remains quite practical for many standard real-world settings.  

• Practical Tips  
  – Combining InterIAMB with a subsequent orientation step or domain expertise is often recommended to interpret directions.  
  – Users handling data with highly non-linear relationships frequently choose a kernel-based, non-linear independence test.  
  – Missing data remain a common pitfall; pre-processing or specialized tests can alleviate performance dips.  

────────────────────────────────────────────────────────
Final Remarks
────────────────────────────────────────────────────────
InterIAMB is a notable variant of the IAMB family, offering iterative refinements that often improve speed and accuracy over the original. Its main hyperparameter, α, is easy to tune based on data size, and its flexible independence test options allow it to handle various data types or noise structures. The algorithm works well for moderate-dimensional problems and moderate levels of missing or noisy data, especially if paired with robust tests and parallel computing resources. However, it does not natively provide edge orientations or handle severe assumption violations, so further post-processing or hybrid approaches may be required for complete causal insights.

────────────────────────────────────────────────────────
Benchmarking Results
────────────────────────────────────────────────────────

• Comparative Performance
  – The benchmarking compared 19 different causal discovery algorithms across multiple scenarios.
  – Each algorithm was evaluated on performance (accuracy), efficiency (runtime), and composite metrics, which are represented as level scores from 1-5, with 5 being the best.
  – Levels scores of composite metrics combine performance and efficiency (weights: 0.8 and 0.2 respectively).
[NOTE] The ranking is smaller, the better. The level score is higher, the better.

• Algorithm Rankings

| Scenario | Rank (Mean) | Rank (Std Dev) | Performance (Level) | Efficiency (Level) | Composite (Level) |
|----------|-----------|---------|-------------|------------|------------|
| Scalability | 5.4 | 1.73 | 4.0 | 4.0 | 4.0 |
| Heterogeneity | 3.2 | 1.09 | 5.0 | 3.0 | 5.0 |
| Measurement Error | 5.0 | 0.00 | 4.0 | 3.0 | 4.0 |
| Noise Type | 6.0 | 1.00 | 4.0 | 3.0 | 4.0 |
| Missing Data | 2.2 | 1.64 | 5.0 | 3.0 | 5.0 |
| Edge Probability | 6.3 | 2.62 | 4.0 | 3.0 | 4.0 |
| Discrete Ratio | 5.0 | 0.00 | 4.0 | 3.0 | 4.0 |

• Analysis

  – Overall mean ranking across 7 scenarios: 4.74
  – Average standard deviation: 1.15



## Supported hyperparameters: {
    "algorithm_name": "InterIAMB",
    "alpha": {
        "meaning": "Desired significance level in (0, 1)",
        "available_values": {
            "default": 0.05,
            "small_sample": 0.1,
            "large_sample": 0.01
        },
        "expert_suggestion": "Adjust based on sample size, more conservative (lower) values for larger samples. If < 500, use 0.1; Else if 500-10000 (<10000 but >500), use 0.05; Else if > 10000, using 0.01."
    },
    "indep_test": {
        "meaning": "Independence test method",
        "available_values": {
            "default": "fisherz",
            "continuous": "fisherz",
            "discrete": "chisq",
            "robust_nonlinear": "kci",
            "fast_robust_nonlinear": "fastkci",
            "approximate_fast_nonlinear": "rcit"
        },
        "expert_suggestion": "Choose based on data type, 'fisherz' for linear continuous data; 'chisq' for discrete data (only applied for pure discrete data); 'kci' for nonlinear data (very slow, use only with variable size < 15 and sample size < 1500); 'fastkci' is faster than kci (use with < 20 variables and sample size < 3000); 'rcit' is the fastest approximation of kci (use with < 30 variables and sample size < 5000)."
    }
}



===============================================

Think in-depth and thoroughly step by step. Please include the reasoning process, the ultimate reason why the picked algorithm beats the others and finally the selected algorithm in the JSON format. Cite/Quote quantity/number and references for the evidences of analyzing using one specific algorithm or not. Do not return any other text or comments:

{
  "reasoning": "reasoning process",
  "reason": "ultimate reason why it beats the others"
  "algorithm": "algorithm_name",
}
