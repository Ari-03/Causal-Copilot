User original query (TOP PRIORITY):
This is a Time-Series dataset, Please discover the causal structure of different factors and profits.

## ⚠️ ESSENTIAL QUERY PRIORITY ⚠️
- User query SUPERSEDES all standard hyperparameter guidelines
- Extract specific needs, constraints, domain insights from user query FIRST
- Parameters MUST be adjusted to meet user's explicit requirements 
- ALL recommendations MUST directly align with the user's stated objectives
- User domain knowledge overrides general optimization guidelines

-----------------------------------------------
Given a dataset with the following properties:

1. Columns: Shopping_Event	Ad_Spend	Page_Views	Unit_Price	Sold_Units	Revenue	Operational_Cost	Profit
2. Statistics:
The dataset has the following characteristics:

Data Type: The overall data type is Time-series.

The sample size is 365 with 8 features. 

This dataset is time-series data. 

Data Quality: There are no missing values in the dataset.

Statistical Properties:
- Linearity: The relationships between variables are not predominantly linear.
- Gaussian Errors: The errors in the data do not follow a Gaussian distribution.
- Time lag: 50 

- Stationarity: The dataset is stationary. 




3. Background Knowledge:
T
h
i
s
 
i
s
 
f
a
k
e
 
d
o
m
a
i
n
 
k
n
o
w
l
e
d
g
e
 
f
o
r
 
d
e
b
u
g
g
i
n
g
 
p
u
r
p
o
s
e
s
.

We have selected the following algorithm for causal discovery:

Algorithm: PCMCI
Description: PCMCI (Peter and Clark algorithm with Momentary Conditional Independence) is a constraint-based causal discovery algorithm designed for time-series data. It is flexible in handling both linear and non-linear relationships and can accommodate non-Gaussian noise. PCMCI is efficient and performs well on small to medium-scale datasets, making it suitable for discovering causal structures in time-series data.
PCMCI is the best choice for this dataset due to its ability to handle time-series data with non-linear relationships and non-Gaussian noise. It provides a CPDAG output, which is acceptable to the user, and is computationally feasible given the dataset's size and characteristics. PCMCI's robustness to violations of assumptions and its successful application in similar domains further support its selection.

Now, we need to determine the optimal hyperparameters for this algorithm. I'll guide you through a systematic approach to select values that prioritize accuracy while maintaining computational efficiency for moderate graph sizes.

Primary hyperparameters to configure: indep_test, tau_min, tau_max, pc_alpha, alpha_level

For each hyperparameter, please follow this structured approach:

Step 1: Understand the dataset characteristics
   - Consider the number of variables (graph size)
   - Analyze sample size and data distribution
   - Note if data is linear/nonlinear, continuous/discrete/mixed
   - For time-series data, prioritize the statistically estimated lag order

Step 2: Assess computational resources
   - Consider the hardware constraints and GPU availability:
   
Current machine doesn't support CUDA, do not choose any GPU-powered algorithms.

   - Prioritize to GPU implemented hyperparameter when dealing with large graph (variable size > 100) to achieve more efficient speedup

Step 3: Evaluate each hyperparameter's impact on accuracy vs. efficiency
   - Critical parameters affecting accuracy (e.g., significance levels, independence tests)
   - Parameters affecting computational complexity (e.g., search depth, maximum conditions)
   - Parameters controlling sparsity (e.g., regularization, thresholds)

Step 4: Analyze algorithm-specific recommendations
   - Review expert suggestions for each parameter:
   "**Parameter:** indep_test\n- **Meaning:** Independence tests\n- **Available Values:**\n  - parcorr\n  - robustparcorr\n  - gpdc\n  - gsq\n  - regression\n  - cmi\n- **Expert Suggestion:** Use parcorr as default. parcorr, gpdc for non-linear causal relations, cmi for better accuracy but slow runtime, gsq for categorical, regression for mixed data\n\n**Parameter:** tau_min\n- **Meaning:** Minimum time lag to consider\n- **Available Values:**\n  - 0\n  - 1\n- **Expert Suggestion:** Use 0 as default. Adjust if results are required for a specific time window\n\n**Parameter:** tau_max\n- **Meaning:** Maximum time lag\n- **Available Values:**\n  - 1\n  - 5\n  - 10\n- **Expert Suggestion:** Use 1 as default. Usually adjusted based on pre-processing results\n\n**Parameter:** pc_alpha\n- **Meaning:** Significance level of PC algorithm.\n- **Available Values:**\n  - 0.05\n  - 0.1\n  - 0.01\n- **Expert Suggestion:** Use 0.05 as default. Adjust based on sample size, more conservative (lower) values for larger samples. If < 500, use 0.1; Else if 500-10000 (<10000 but >500), use 0.05; Else if > 10000, using 0.01.\n\n**Parameter:** alpha_level\n- **Meaning:** Significance level at which the p_matrix is thresholded to get graph.\n- **Available Values:**\n  - 0.05\n  - 0.1\n  - 0.01\n- **Expert Suggestion:** Use 0.05 as default. Adjust based on estimated time lag, number of nodes and sample size. Larger and denser graph means low alpha value.\n\n"

Step 5: Make final decisions based on:
   - For moderate graph sizes (<50 variables), prioritize accuracy over speed
   - For large graphs (>50 variables), balance accuracy with feasibility and EFFICIENCY
   - For time-series data, carefully consider temporal parameters

Please provide your suggestions in a structured JSON format, with detailed reasoning for each hyperparameter. Your response should look like this:

{
  "algorithm": "PCMCI",
  "hyperparameters": {
    "[HYPERPARAMETER_1_NAME]": {
      "full_name": "[HYPERPARAMETER_1_FULL_NAME]",
      "reasoning": "[YOUR_STEP_BY_STEP_REASONING_PROCESS]",
      "value": [SUGGESTED_VALUE],
      "explanation": "[BRIEF_EXPLANATION_OF_TRADEOFFS]"
    },
    "[HYPERPARAMETER_2_NAME]": {
      "full_name": "[HYPERPARAMETER_2_FULL_NAME]",
      "reasoning": "[YOUR_STEP_BY_STEP_REASONING_PROCESS]",
      "value": [SUGGESTED_VALUE],
      "explanation": "[BRIEF_EXPLANATION_OF_TRADEOFFS]"
    }
  }
}

Important guidelines:
1. Only select values from the "available_values" list for each hyperparameter
2. For moderate graph sizes (10-50 variables), prioritize accuracy over speed
3. For time-series data, give special attention to lag parameters based on statistical estimates
4. Consider independence test selection carefully based on data type and computational resources
5. For regularization parameters, consider the expected graph density
6. For search depth parameters, consider the complexity of potential causal relationships

Please provide your hyperparameter suggestions following this JSON structure, with clear reasoning that demonstrates you've considered the dataset characteristics, algorithm requirements, and computational constraints.