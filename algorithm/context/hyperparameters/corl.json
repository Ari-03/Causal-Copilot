{
  "algorithm_name": "CORL",
  "batch_size": {
    "meaning": "Training batch size",
    "suggested_values": {
      "default": 64,
      "small_graph": 32,
      "medium_graph": 64,
      "large_graph": 128
    },
    "reasoning": "Should be greater than or equal to number of nodes. Increase for larger graphs to improve stability."
  },
  "embed_dim": {
    "meaning": "Dimension of embedding layer output",
    "suggested_values": {
      "default": 256,
      "small_graph": 128,
      "medium_graph": 256,
      "large_graph": 512
    },
    "reasoning": "Higher dimensions can capture more complex patterns but require more computation."
  },
  "reward_mode": {
    "meaning": "Type of reward mechanism",
    "suggested_values": {
      "default": "episodic",
      "alternatives": ["dense", "episodic"]
    },
    "reasoning": "Use 'episodic' for better exploration, 'dense' for faster convergence."
  },
  "reward_score_type": {
    "meaning": "Score function for reward calculation",
    "suggested_values": {
      "default": "BIC",
      "alternatives": ["BIC", "BDeu"]
    },
    "reasoning": "BIC is more general, BDeu is better for discrete data."
  },
  "iteration": {
    "meaning": "Number of training iterations",
    "suggested_values": {
      "default": 5000,
      "small_graph": 3000,
      "medium_graph": 5000,
      "large_graph": 10000
    },
    "reasoning": "More iterations needed for larger graphs. Increase if results are unstable."
  },
  "actor_lr": {
    "meaning": "Learning rate for actor network",
    "suggested_values": {
      "default": 1e-4,
      "conservative": 5e-5,
      "aggressive": 5e-4
    },
    "reasoning": "Lower values for more stable training, higher values for faster convergence."
  },
  "critic_lr": {
    "meaning": "Learning rate for critic network",
    "suggested_values": {
      "default": 1e-3,
      "conservative": 5e-4,
      "aggressive": 5e-3
    },
    "reasoning": "Usually set higher than actor_lr. Adjust based on training stability."
  }
} 