## Available Algorithms

Current available algorithms implemented for usage:

- DML 
- LinearDML
- SparseLinearDML
- CausalForestDML
- Metalearners

## Domain Knowledge about Algorithm Selection

Based on the characteristics of your data and requirements, consider the following priority order for selecting causal discovery algorithms:

1. If your data is nonstationary or heterogeneous across domains/time:
   - Use CDNOD as the first choice
   - Note that DO NOT use it if your data is not heterogeneous or nonstationary
   
2. If your data is linear or you prefer a score-based approach and assume no hidden confounders:
   - Consider GES (Greedy Equivalence Search)
   
3. If the noise is non-Gaussian and you believe the relationships are linear:
   - Try DirectLiNGAM first
   - If computational resources allow, also consider ICALiNGAM
   - Note that DO NOT use them if there are non-linear relations in your data.

4. If you have a large dataset with all relevant variables observed:
   - Start with PC algorithm
   
5. If your data is high-dimensional and you prefer a continuous optimization approach:
   - Experiment with NOTEARS
   
6. If you suspect the presence of hidden confounders:
   - Try FCI as the primary algorithm

Additional considerations:

- For large datasets where efficiency is crucial, prioritize PC or GES
- If you need a fully directed graph rather than a Markov equivalence class, prefer LiNGAM variants or NOTEARS
- When dealing with non-linear relationships, consider extensions of these algorithms designed for non-linear data

Default algorithm ranking for general cases (when data doesn't favor any specific characteristics):

1. PC: Good balance between generality and computational efficiency
2. GES: Efficient for larger datasets and provides a good general approach
3. FCI: More general than PC but computationally more intensive
4. NOTEARS: Efficient for high-dimensional data but assumes linear relationships
5. DirectLiNGAM: Efficient but assumes linear relationships and non-Gaussian noise
6. ICALiNGAM: More computationally intensive than DirectLiNGAM
7. CDNOD: Specialized for nonstationary/heterogeneous data, may be overkill for stationary data
8. Metalearners: Flexible and data-adaptive, suitable for complex and nonlinear relationships

## Algorithms Description 

### DML

- **Description**:  
  Double Machine Learning (DML) is a framework for estimating treatment effects using a two-stage process: nuisance functions are estimated in the first stage, and treatment effects are estimated in the second stage using residual-on-residual regression.

- **Assumptions**:  
  1. The result variable Y has a linear parametric form with treatment T
  2. Treatments can be continuous or discrete.  

- **Advantages**:  
  1. Handles multiple treatments, outcomes, and high-dimensional features.  
  2. Supports flexible scikit-learn-compatible models for nuisance functions and final regression.  

- **Limitations**:  
  1. Relies on linear parametric assumptions, which may not capture nonlinear relationships.  
  2. Can not provide confidence intervals
  3. Cross-fitting increases computational complexity.  

- **Suitable Cases**:  
  1. Estimating treatment effects with linear relationships or approximations.  
  2. Scenarios with multiple treatments or outcomes.  
  3. High-dimensional data where regularized linear regression is applicable.  

### LinearDML

- **Description**:  
  LinearDML is a Double Machine Learning estimator with a low-dimensional linear parametric final stage, implemented using a statsmodels regression.

- **Assumptions**:  
  1. The CATE is linear in the features, modeled explicitly as a low-dimensional linear regression.  

- **Advantages**:  
  1. Simple and fast 
  2. Can provide confidence intervals

- **Limitations**:  
  1. Limited to low-dimensional feature spaces in the final stage due to the linear regression assumption.  
  2. May underperform when the true treatment effect is nonlinear.  

- **Suitable Cases**:  
  1. When treatment effects are expected to have a linear relationship with features.  
  2. Scenarios with low-dimensional feature spaces in the final stage.
  3. Applications requiring confidence intervals or hypothesis testing for treatment effects.
  

### SparseLinearDML

- **Description**:  
  SparseLinearDML is a specialized version of the DML estimator designed for high-dimensional features and sparse dataset.

- **Assumptions**:  
  1. The CATE is linear in the features, with sparse coefficients.   
  2. The high-dimensional feature space can be effectively regularized to exploit sparsity.  

- **Advantages**:  
  1. Handles high-dimensional feature spaces effectively using regularization techniques.  
  2. Provides interpretable results by focusing on sparse, non-zero coefficients.  

- **Limitations**:  
  1. Relies on the sparsity assumption, which may not hold in all datasets.  
  2. Requires careful tuning of regularization parameters (e.g., Lasso or other sparsity-inducing techniques).  

- **Suitable Cases**:  
  1. When the treatment effect is expected to be linear but only depends on a sparse subset of high-dimensional features.  
  2. High-dimensional data where standard linear models may overfit without regularization.  
  3. Applications requiring confidence intervals or hypothesis testing for treatment effects.

### CausalForestDML

- **Description**:  
  CausalForestDML combines a causal forest algorithm with DML-based residualization of treatment and outcome variables to estimate flexible, non-linear models of heterogeneous treatment effects.

- **Assumptions**:   
  1. The data-generating process may have low-dimensional latent structures, even if the observed feature space is high-dimensional.  
  2. The moment equation is satisfied locally by the causal forest.

- **Advantages**:  
  1. Captures non-linear and flexible heterogeneity in treatment effects.  
  2. Adapts to high-dimensional features without requiring strong parametric assumptions.  
  3. Provides valid confidence intervals, even when using data-adaptive methods.  
  4. Performs well in scenarios with many features and complex heterogeneity structures.  

- **Limitations**:  
  1. Computationally intensive, especially with large datasets or many features.  
  2. Requires a sufficient number of samples to perform non-parametric estimation effectively.  

- **Suitable Cases**:  
  1. High-dimensional feature spaces where the heterogeneity of the treatment effect is unknown or complex.  
  2. Scenarios requiring flexible, non-parametric models of treatment effects.  
  3. Applications where valid confidence intervals are critical despite data adaptivity.

### Metalearners

- **Description**:  
  Metalearners are highly flexible frameworks for estimating CATE, allowing users to specify any machine learning method at each stage of the estimation process, including cross-validation for model selection.

- **Assumptions**:  
  1. No strong assumptions on the form of CATE, as the method is flexible and data-adaptive.  
  2. The quality of the CATE estimate depends on the performance of the chosen ML models for nuisance functions and the final stage.  

- **Advantages**:  
  1. Provides full flexibility to use any ML method at each stage, enabling powerful and adaptive modeling.  
  2. Supports cross-validation for automatic model selection, reducing the risk of overfitting.  
  3. Aims to minimize mean squared error (MSE) of the CATE estimate.  

- **Limitations**:  
  1. Does not provide valid confidence intervals due to the unrestricted flexibility and unclear trade-offs between bias and variance.  
  2. Performance highly depends on the choice of ML models and hyperparameters.  
  3. Computationally intensive when combining cross-validation with complex ML models.  

- **Suitable Cases**:  
  1. Scenarios where minimizing the MSE of the CATE estimate is the primary goal.  
  2. When flexibility is needed to experiment with different ML models and perform model selection.  
  3. Use cases where confidence intervals are not a critical requirement.

#### S-Learner

- **Description**:  
  The S-Learner estimates treatment effects by fitting a single model to the entire dataset, including the treatment variable as a feature. It is simple and works well when the treatment effect is relatively homogeneous.

- **Assumptions**:  
  1. The treatment effect can be captured by including the treatment variable as a feature in a single model.  
  2. The relationship between covariates, treatment, and outcome is well-specified by the chosen machine learning model.  

- **Advantages**:  
  1. Simple to implement and computationally efficient.  
  2. Works well when the treatment effect is not highly heterogeneous.  
  3. Can handle multiple treatments and continuous treatment variables.  

- **Limitations**:  
  1. May struggle with highly heterogeneous treatment effects.  
  2. The treatment effect is inferred indirectly, which can lead to bias if the model is misspecified.  
  3. Less flexible than other metalearners for capturing complex treatment effect heterogeneity.  

- **Suitable Cases**:  
  1. When the treatment effect is expected to be relatively uniform across the population.  
  2. Scenarios where computational efficiency is a priority.  
  3. Use cases where a single model can adequately capture the relationship between treatment and outcome.

#### T-Learner

- **Description**:  
  The T-Learner estimates treatment effects by fitting separate models for the treated and control groups. It is more flexible than the S-Learner and can capture heterogeneous treatment effects.

- **Assumptions**:  
  1. The treated and control groups can be modeled separately.  
  2. The relationship between covariates and outcomes is well-specified by the chosen machine learning models.  

- **Advantages**:  
  1. Can capture heterogeneous treatment effects better than the S-Learner.  
  2. Simple to implement and interpret.  
  3. Works well when the treated and control groups have similar covariate distributions.  

- **Limitations**:  
  1. Requires fitting two models, which can be computationally expensive.  
  2. May overfit if the treated and control groups have very different distributions.  
  3. Does not explicitly model the treatment effect, which can lead to inefficiencies.  

- **Suitable Cases**:  
  1. When the treatment effect is expected to vary across subgroups.  
  2. Scenarios where the treated and control groups have similar covariate distributions.  
  3. Use cases where capturing heterogeneity in treatment effects is important.

#### X-Learner

- **Description**:  
  The X-Learner is an extension of the T-Learner that uses information from both the treated and control groups to improve the estimation of treatment effects. It is particularly effective when the treatment and control groups are imbalanced.

- **Assumptions**:  
  1. The treated and control groups can be modeled separately, and their predictions can be used to improve treatment effect estimation.  
  2. The relationship between covariates and outcomes is well-specified by the chosen machine learning models.  

- **Advantages**:  
  1. Handles imbalanced treatment and control groups effectively.  
  2. Can provide more accurate estimates of heterogeneous treatment effects.  
  3. Combines the strengths of the S-Learner and T-Learner.  

- **Limitations**:  
  1. More complex to implement than S-Learner and T-Learner.  
  2. Computationally intensive due to the need to fit multiple models.  
  3. Requires careful tuning of the models for treated and control groups.  

- **Suitable Cases**:  
  1. When the treatment and control groups are imbalanced.  
  2. Scenarios where accurate estimation of heterogeneous treatment effects is critical.  
  3. Use cases where combining information from treated and control groups can improve estimation.

#### Domain Adaptation Learner

- **Description**:  
  The Domain Adaptation Learner is designed for scenarios where the treated and control groups come from different domains (e.g., different time periods or geographic regions). It uses domain adaptation techniques to account for differences in the distributions of the treated and control groups.

- **Assumptions**:  
  1. The treated and control groups come from different domains with different covariate distributions.  
  2. Domain adaptation techniques can effectively account for these differences.  

- **Advantages**:  
  1. Effective in handling domain shifts between treated and control groups.  
  2. Can improve the accuracy of treatment effect estimates in cross-domain settings.  
  3. Flexible and can incorporate various domain adaptation techniques.  

- **Limitations**:  
  1. Requires domain adaptation techniques, which can be complex to implement.  
  2. Computationally intensive due to the need to account for domain differences.  
  3. Performance depends on the quality of the domain adaptation method used.  

- **Suitable Cases**:  
  1. When the treated and control groups come from different domains (e.g., different time periods or regions).  
  2. Scenarios where domain shifts are a concern.  
  3. Use cases where accurate treatment effect estimation across domains is critical.